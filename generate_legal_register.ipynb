{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjawad/AI-for-Environmental-Regulation-Analysis/blob/main/generate_legal_register.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "usOlcCMmlmvL",
        "outputId": "8883bdce-cd14-4f7b-d222-6c3121d8718c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab) (3.4.2)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab, PyPDF2, pypdf, pymupdf\n",
            "Successfully installed PyPDF2-3.0.1 pymupdf-1.26.3 pypdf-5.9.0 reportlab-4.4.3\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf reportlab PyPDF2 pypdf\n",
        "!pip install --upgrade transformers accelerate  --quiet\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lHL6gdkbloUY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import json\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "f0b18912528a4dd7b596c9fc2579eb69"
          ]
        },
        "id": "xlh4nOUelrtX",
        "outputId": "f6487e24-ba8c-4c27-c720-5f13e2116637"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0b18912528a4dd7b596c9fc2579eb69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EbLzKOOCltjF",
        "outputId": "5cb08f78-c775-4600-fbb2-c0de0577d8c1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BitsAndBytesConfig' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4122560000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m bnb_config = BitsAndBytesConfig(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbnb_4bit_quant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nf4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined"
          ]
        }
      ],
      "source": [
        "# @title ← 3. Exécutez cette cellule pour charger le modèle \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.1,\n",
        "    do_sample=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hkdpJR2eluSx",
        "outputId": "eb7c054a-9428-4014-c710-33d8a7e70720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=================================================\n",
            "Traitement du fichier : BO_7410_fr.pdf\n",
            "=================================================\n",
            "--- Carte du Sommaire (14 actes) ---\n",
            "  -> Trouvé 'Dahir n° 1-25-39 du 12 chaoual 1446 (11 avril 2025), portant nomination de la présidente du [...]' page 2032\n",
            "  -> Trouvé 'Dahir n° 1-25-40 du 12 chaoual 1446 (11 avril 2025) portant renouvellement de nomination de la [...]' page 2032\n",
            "  -> Trouvé 'Dahir n° 1-25-41 du 12 chaoual 1446 (11 avril 2025) portant nomination du délégué [...]' page 2032\n",
            "  -> Trouvé 'Dahir n° 1-25-46 du 17 kaada 1446 (15 mai 2025) portant nomination du directeur général du [...]' page 2032\n",
            "  -> Trouvé 'Dahir n° 1-25-47 du 17 kaada 1446 (15 mai 2025) portant nomination du directeur général de [...]' page 2032\n",
            "  -> Trouvé 'Dahir n° 1-24-12 du 10 chaabane 1445 (20 février 2024) portant promulgation de la loi n° 01-22 [...]' page 2033\n",
            "  -> Trouvé 'Décret n°2-25-366 du 24 kaada 1446 (22 mai 2025) relatif à la navigation aérienne militaire' page 2040\n",
            "  -> Trouvé 'Décret n° 2-25-269 du 21 kaada 1446 (19 mai 2025) approuvant l’accord de prêt n° 97560-MA d’un [...]' page 2041\n",
            "  -> Trouvé 'Décret n° 2-25-370 du 24 kaada 1446 (22 mai 2025) approuvant l’accord de prêt n° [...]' page 2042\n",
            "  -> Trouvé 'Arrêté du ministre de l’agriculture, de la pêche maritime, du développement rural et des eaux [...]' page 2043\n",
            "  -> Trouvé 'Arrêté de la ministre de l’économie et des finances n° 1149-25 du 9 kaada 1446 (7 mai 2025) [...]' page 2050\n",
            "  -> Trouvé 'Arrêté du ministre de la santé et de la protection sociale n° 1218-25 du 11 kaada 1446 (9 mai [...]' page 2051\n",
            "  -> Trouvé 'Arrêté conjoint du ministre de l’industrie et du commerce et de la ministre de l’économie et [...]' page 2055\n",
            "  -> Trouvé 'Arrêté du ministre de l’enseignement supérieur, de la recherche scientifique et de [...]' page 2056\n",
            "\n",
            "Décalage de page : 2032\n",
            "\n",
            "--- 14 actes extraits ---\n",
            "  - Dahir: Dahir n° 1-25-39 du 12 chaoual 1446 (11 avril 2025), portant nomination de la présidente du [...]\n",
            "  - Dahir: Dahir n° 1-25-40 du 12 chaoual 1446 (11 avril 2025) portant renouvellement de nomination de la [...]\n",
            "  - Dahir: Dahir n° 1-25-41 du 12 chaoual 1446 (11 avril 2025) portant nomination du délégué [...]\n",
            "  - Dahir: Dahir n° 1-25-46 du 17 kaada 1446 (15 mai 2025) portant nomination du directeur général du [...]\n",
            "  - Dahir: Dahir n° 1-25-47 du 17 kaada 1446 (15 mai 2025) portant nomination du directeur général de [...]\n",
            "  - Dahir: Dahir n° 1-24-12 du 10 chaabane 1445 (20 février 2024) portant promulgation de la loi n° 01-22 [...]\n",
            "  - Décret: Décret n°2-25-366 du 24 kaada 1446 (22 mai 2025) relatif à la navigation aérienne militaire\n",
            "  - Décret: Décret n° 2-25-269 du 21 kaada 1446 (19 mai 2025) approuvant l’accord de prêt n° 97560-MA d’un [...]\n",
            "  - Décret: Décret n° 2-25-370 du 24 kaada 1446 (22 mai 2025) approuvant l’accord de prêt n° [...]\n",
            "  - Arrêté: Arrêté du ministre de l’agriculture, de la pêche maritime, du développement rural et des eaux [...]\n",
            "  - Arrêté: Arrêté de la ministre de l’économie et des finances n° 1149-25 du 9 kaada 1446 (7 mai 2025) [...]\n",
            "  - Arrêté: Arrêté du ministre de la santé et de la protection sociale n° 1218-25 du 11 kaada 1446 (9 mai [...]\n",
            "  - Arrêté: Arrêté conjoint du ministre de l’industrie et du commerce et de la ministre de l’économie et [...]\n",
            "  - Arrêté: Arrêté du ministre de l’enseignement supérieur, de la recherche scientifique et de [...]\n",
            "\n",
            "✅ Toutes les données ont été sauvegardées dans 'data_preprocessed_BULLETINS.json'\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import json\n",
        "import unicodedata\n",
        "import textwrap\n",
        "\n",
        "# --- Fonctions utilitaires ---\n",
        "def clean_text(text):\n",
        "    if not text: return \"\"\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# --- FONCTION CORRIGÉE ---\n",
        "def get_page_offset(doc, toc_map):\n",
        "    \"\"\"\n",
        "    Calcule l'offset de manière fiable en testant les premiers actes du sommaire.\n",
        "    \"\"\"\n",
        "    if not toc_map:\n",
        "        raise ValueError(\"La table des matières est vide, impossible de calculer l'offset.\")\n",
        "\n",
        "    # On teste les 3 premiers actes du sommaire pour plus de robustesse\n",
        "    for act_to_find in toc_map[:3]:\n",
        "        try:\n",
        "            printed_page_num = act_to_find['start_page_in_doc']\n",
        "\n",
        "            # Utiliser une partie unique du titre pour la recherche\n",
        "            search_snippet = clean_text(act_to_find['legal_requirement'].split('portant')[0])\n",
        "            if len(search_snippet) < 20 or \"n°\" not in search_snippet:\n",
        "                search_snippet = clean_text(act_to_find['legal_requirement'])[:60]\n",
        "\n",
        "            pattern = re.compile(re.escape(search_snippet), re.IGNORECASE)\n",
        "\n",
        "            for i, page in enumerate(doc):\n",
        "                page_text = clean_text(page.get_text(\"text\"))\n",
        "                if pattern.search(page_text):\n",
        "                    # Si on trouve, on a notre offset et on quitte la fonction\n",
        "                    return printed_page_num - i\n",
        "        except Exception:\n",
        "            # Si un acte échoue, on continue simplement avec le suivant\n",
        "            continue\n",
        "\n",
        "    raise ValueError(f\"Impossible de localiser les premiers actes du sommaire dans le document pour calculer l'offset.\")\n",
        "\n",
        "\n",
        "def map_table_of_contents_simplified(doc):\n",
        "    \"\"\"\n",
        "    Scanne le texte des premières pages pour trouver les entrées du sommaire.\n",
        "    \"\"\"\n",
        "    toc_map = []\n",
        "    full_toc_text = \"\"\n",
        "    for i in range(min(2, len(doc))):\n",
        "        full_toc_text += doc[i].get_text(\"text\")\n",
        "\n",
        "    full_toc_text = clean_text(full_toc_text)\n",
        "\n",
        "    # Regex amélioré pour capturer les titres, même s'ils contiennent eux-mêmes des points\n",
        "    pattern = re.compile(\n",
        "        r\"((?:Arrêté|Décision|Décret|Loi|Dahir|Avis)\\s+.*?n°\\s*[\\w\\.\\-]+.*?)\\s*\\.{5,}\\s*(\\d{3,})\",\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    matches = pattern.findall(full_toc_text)\n",
        "\n",
        "    for match in matches:\n",
        "        title = clean_text(match[0])\n",
        "        page_number = int(match[1])\n",
        "        category_match = re.search(r\"^(Arrêté|Décision|Décret|Loi|Dahir|Avis)\", title, re.IGNORECASE)\n",
        "        category = category_match.group(1).capitalize() if category_match else \"Inconnu\"\n",
        "        toc_map.append({\n",
        "            \"legal_requirement\": title,\n",
        "            \"category\": category,\n",
        "            \"start_page_in_doc\": page_number,\n",
        "        })\n",
        "\n",
        "    toc_map.sort(key=lambda x: x['start_page_in_doc'])\n",
        "\n",
        "    unique_map = []\n",
        "    seen_titles = set()\n",
        "    for item in toc_map:\n",
        "        if item['legal_requirement'] not in seen_titles:\n",
        "            unique_map.append(item)\n",
        "            seen_titles.add(item['legal_requirement'])\n",
        "\n",
        "    return unique_map\n",
        "\n",
        "def extract_full_content_from_map(doc, toc_map, page_offset):\n",
        "    \"\"\"Extrait le contenu complet en se basant sur la carte du sommaire.\"\"\"\n",
        "    final_acts = []\n",
        "    num_pages_in_doc = len(doc)\n",
        "\n",
        "    for i, act in enumerate(toc_map):\n",
        "        start_index = act['start_page_in_doc'] - page_offset\n",
        "\n",
        "        if i + 1 < len(toc_map):\n",
        "            next_act_start_page = toc_map[i + 1]['start_page_in_doc']\n",
        "            end_index = next_act_start_page - page_offset\n",
        "        else:\n",
        "            end_index = num_pages_in_doc\n",
        "\n",
        "        start_index = max(0, start_index)\n",
        "        end_index = min(num_pages_in_doc, end_index)\n",
        "\n",
        "        content_parts = []\n",
        "        if start_index < end_index:\n",
        "            for page_num in range(start_index, end_index):\n",
        "                content_parts.append(doc[page_num].get_text(\"text\", sort=True))\n",
        "        elif 0 <= start_index < num_pages_in_doc:\n",
        "             content_parts.append(doc[start_index].get_text(\"text\", sort=True))\n",
        "\n",
        "        full_content = \" \".join(content_parts)\n",
        "        cleaned_content = clean_text(full_content)\n",
        "\n",
        "        act['full_text_content'] = cleaned_content\n",
        "        final_acts.append(act)\n",
        "\n",
        "    return final_acts\n",
        "\n",
        "# --- SCRIPT PRINCIPAL ---\n",
        "if __name__ == '__main__':\n",
        "    pdf_files = [\n",
        "        'BO_7410_fr.pdf',\n",
        "    ]\n",
        "\n",
        "    all_bulletins_data = {}\n",
        "\n",
        "    for pdf_file_path in pdf_files:\n",
        "        print(f\"\\n=================================================\")\n",
        "        print(f\"Traitement du fichier : {pdf_file_path}\")\n",
        "        print(f\"=================================================\")\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file_path)\n",
        "\n",
        "            table_of_contents = map_table_of_contents_simplified(doc)\n",
        "\n",
        "            if not table_of_contents:\n",
        "                print(\"ERREUR : Le sommaire n'a pas pu être parsé pour ce fichier.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"--- Carte du Sommaire ({len(table_of_contents)} actes) ---\")\n",
        "            for item in table_of_contents:\n",
        "                print(f\"  -> Trouvé '{textwrap.shorten(item['legal_requirement'], 100)}' page {item['start_page_in_doc']}\")\n",
        "\n",
        "            # --- LIGNE CORRIGÉE ---\n",
        "            # On passe la liste entière à la fonction `get_page_offset` qui sait maintenant la gérer.\n",
        "            page_offset = get_page_offset(doc, table_of_contents)\n",
        "            print(f\"\\nDécalage de page : {page_offset}\\n\")\n",
        "\n",
        "            final_data = extract_full_content_from_map(doc, table_of_contents, page_offset)\n",
        "\n",
        "            print(f\"--- {len(final_data)} actes extraits ---\")\n",
        "            for act in final_data:\n",
        "                print(f\"  - {act['category']}: {textwrap.shorten(act['legal_requirement'], 100)}\")\n",
        "\n",
        "            all_bulletins_data[pdf_file_path] = final_data\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             print(f\"ERREUR : Fichier non trouvé '{pdf_file_path}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Une erreur est survenue lors du traitement de {pdf_file_path} : {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    output_filename = 'data_preprocessed_BULLETINS.json'\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_bulletins_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"\\n✅ Toutes les données ont été sauvegardées dans '{output_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYKtgsHulwhL"
      },
      "outputs": [],
      "source": [
        "# --- Étape 2 : Fonctions d'Analyse avec le Bon Template de Prompt ---\n",
        "\n",
        "def call_llm_mistral(prompt):\n",
        "    \"\"\"\n",
        "    Formate le prompt pour Mistral Instruct, appelle le pipeline local,\n",
        "    et nettoie la réponse pour extraire le JSON.\n",
        "    \"\"\"\n",
        "    # Template correct pour Mistral Instruct\n",
        "    formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
        "\n",
        "    try:\n",
        "        output = text_generator(formatted_prompt)\n",
        "        raw_response = output[0]['generated_text']\n",
        "        # Extraire uniquement le texte généré par le modèle après le prompt\n",
        "        cleaned_response = raw_response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "        # Tentative d'extraire le JSON du texte\n",
        "        # Le modèle peut parfois ajouter des commentaires avant/après le JSON\n",
        "        json_match = re.search(r'\\{.*\\}', cleaned_response, re.DOTALL)\n",
        "        if json_match:\n",
        "            return json.loads(json_match.group(0))\n",
        "        else:\n",
        "            print(f\"  -> Avertissement : Le LLM n'a pas retourné un JSON valide. Réponse brute : {cleaned_response}\")\n",
        "            return None # Retourne None en cas d'échec\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"  -> Erreur : Impossible de décoder le JSON de la réponse du LLM. Réponse brute : {cleaned_response}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"  -> Erreur lors de l'appel LLM : {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_legal_act(act_data, project_description):\n",
        "    \"\"\"\n",
        "    Analyse un acte juridique avec un prompt CoT finalisé qui insiste\n",
        "    lourdement sur le format de sortie JSON.\n",
        "    \"\"\"\n",
        "    print(f\"\\n  -> Analyse de : {textwrap.shorten(act_data['legal_requirement'], 100)}\")\n",
        "\n",
        "    # --- PROMPT FINAL AVEC INSTRUCTION DE FORMATAGE RENFORCÉE ---\n",
        "    prompt = f\"\"\"\n",
        "    Tu es un assistant expert en conformité réglementaire. Ta tâche est d'analyser un texte de loi et de le classer pour un projet industriel.\n",
        "\n",
        "    **Contexte du Projet:**\n",
        "    {project_description}\n",
        "\n",
        "    **Texte de Loi à Analyser:**\n",
        "    - Titre : {act_data['legal_requirement']}\n",
        "    - Contenu :\n",
        "    \\\"\\\"\\\"\n",
        "    {textwrap.shorten(act_data['full_text_content'], 4000, placeholder='...')}\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    **Instructions:**\n",
        "    Suis ce processus de raisonnement pour arriver à ta réponse finale :\n",
        "    1.  **Résumé et Pertinence :** D'abord, résume le texte. Ensuite, détermine si le sujet a un impact DIRECT sur une usine chimique (construction, opération, sécurité, environnement, permis).\n",
        "    2.  **Classification :** Si le texte est pertinent, choisis la catégorie la plus appropriée dans cette liste : ['General Environmental & Sustainability Regulations', 'Solid Waste & auxilary products', 'Water and Liquid discharges', 'Noise and vibrations', 'Energy', 'Air', 'Hazardeous materials', 'Occupational Health and safety']. Si non pertinent, la catégorie est \"Non Applicable\".\n",
        "    3.  **Remplissage des Champs :** Remplis tous les champs requis en te basant sur ton analyse. Pour les champs avec des choix contraints, utilise UNIQUEMENT les options fournies.\n",
        "\n",
        "    **Réponse Finale:**\n",
        "    Après avoir suivi ces étapes de raisonnement, fournis ta réponse finale. Ta réponse doit être **uniquement et exclusivement un dictionnaire JSON valide**. N'ajoute aucun texte, aucune explication, ni de numérotation avant ou après le JSON.\n",
        "\n",
        "    **Format JSON Obligatoire:**\n",
        "    {{\n",
        "      \"category_title\": \"...\",\n",
        "      \"description\": \"...\",\n",
        "      \"phase\": \"['All project phases', 'Pre-construction', 'Evaluate/Define', 'Construction/Operation']\",\n",
        "      \"activity_aspect\": \"...\",\n",
        "      \"impacts\": \"['Environment', 'Health & Safety', 'Regulatory & Permitting']\",\n",
        "      \"type\": \"['Regulation', 'Guideline', 'Standard']\",\n",
        "      \"task\": \"...\",\n",
        "      \"responsibility\": \"...\",\n",
        "      \"comments\": \"...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # On utilise votre fonction existante pour appeler le modèle\n",
        "    analysis_result = call_llm_mistral(prompt)\n",
        "    return analysis_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW8nc4SLl03R"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    input_json_path = 'data_preprocessed_BULLETINS.json'\n",
        "\n",
        "    project_description = \"\"\"\n",
        "    Projet de construction d'une usine d'additifs chimiques pour le client NOVADDIX.\n",
        "    L'usine produira 20 000 tonnes/an d'additifs.\n",
        "    Domaine : Industriel - Chimie. Classe de produit : Produits inflammables et toxiques.\n",
        "    \"\"\"\n",
        "    print(\"STARTING\")\n",
        "    try:\n",
        "        with open(input_json_path, 'r', encoding='utf-8') as f:\n",
        "            all_bulletins_data = json.load(f)\n",
        "        print(f\"Fichier de données '{input_json_path}' chargé avec succès.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERREUR : Le fichier d'entrée '{input_json_path}' n'a pas été trouvé.\")\n",
        "        exit()\n",
        "\n",
        "    all_analyzed_acts = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for pdf_file, legal_acts in all_bulletins_data.items():\n",
        "        print(f\"\\n--- Traitement des actes de : {pdf_file} ---\")\n",
        "        for act in legal_acts:\n",
        "            llm_analysis = analyze_legal_act(act, project_description)\n",
        "\n",
        "            if llm_analysis:\n",
        "                # On ajoute les données initiales (parsing) aux données analysées (LLM)\n",
        "                full_entry = {\n",
        "                    \"Legal Requirement\": act['legal_requirement'],\n",
        "                    \"Date\": re.search(r'\\((\\d{1,2}\\s+\\w+\\s+\\d{4})\\)', act['legal_requirement']).group(1) if re.search(r'\\((\\d{1,2}\\s+\\w+\\s+\\d{4})\\)', act['legal_requirement']) else 'N/A',\n",
        "                    \"Type\": act['category'],\n",
        "                    \"Jurisdiction\": \"National\",\n",
        "                    \"CategoryTitle\": llm_analysis.get(\"category_title\", \"Uncategorized\"), # NOUVELLE CLÉ POUR LE GROUPEMENT\n",
        "                    \"Description\": llm_analysis.get(\"description\", \"N/A\"),\n",
        "                    \"Phase\": llm_analysis.get(\"phase\", \"N/A\"),\n",
        "                    \"Activity/Aspect\": llm_analysis.get(\"activity_aspect\", \"N/A\"),\n",
        "                    \"Impacts\": llm_analysis.get(\"impacts\", \"N/A\"),\n",
        "                    \"Task\": llm_analysis.get(\"task\", \"N/A\"),\n",
        "                    \"Responsibility\": llm_analysis.get(\"responsibility\", \"N/A\"),\n",
        "                    \"Comments\": llm_analysis.get(\"comments\", \"N/A\"),\n",
        "                }\n",
        "                all_analyzed_acts.append(full_entry)\n",
        "            time.sleep(1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n--- Analyse LLM terminée en {end_time - start_time:.2f} secondes ---\")\n",
        "\n",
        "    if all_analyzed_acts:\n",
        "        # Création du DataFrame avec toutes les données\n",
        "        df = pd.DataFrame(all_analyzed_acts)\n",
        "        try:\n",
        "            output_csv_file = 'registre_legal_output.csv'\n",
        "            df.to_csv(output_csv_file, index=False, encoding='utf-8-sig')\n",
        "            print(f\"\\nFichier CSV généré avec succès : '{output_csv_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nErreur lors de la génération du fichier CSV : {e}\")\n",
        "\n",
        "        structured_data_for_pdf = []\n",
        "        # On groupe par la nouvelle clé 'CategoryTitle' générée par le LLM\n",
        "        grouped = df.groupby('CategoryTitle')\n",
        "\n",
        "        for category_name, group in grouped:\n",
        "            category_dict = {\n",
        "                \"category_title\": category_name,\n",
        "                \"rows\": []\n",
        "            }\n",
        "\n",
        "            for index, row in group.iterrows():\n",
        "                # Créer la liste de chaînes dans le bon ordre pour le PDF\n",
        "                row_list = [\n",
        "                    row['Phase'], row['Activity/Aspect'], row['Impacts'],\n",
        "                    row['Jurisdiction'], row['Type'], row['Legal Requirement'],\n",
        "                    row['Date'], row['Description'], row['Task'],\n",
        "                    row['Responsibility'], row['Comments']\n",
        "                ]\n",
        "                category_dict[\"rows\"].append(row_list)\n",
        "\n",
        "            structured_data_for_pdf.append(category_dict)\n",
        "\n",
        "        print(f\"\\nDonnées transformées en {len(structured_data_for_pdf)} catégories pour le PDF.\")\n",
        "    else:\n",
        "        print(\"\\nAucune donnée n'a été générée pour le registre légal final.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTc99oAxl29u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from reportlab.platypus  import SimpleDocTemplate, Table, TableStyle, Paragraph\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.pagesizes import landscape, A3\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.enums import TA_CENTER, TA_LEFT\n",
        "\n",
        "# --- FONCTION DE GÉNÉRATION DE PDF (maintenant elle accepte les données) ---\n",
        "\n",
        "def generate_legal_register_pdf(structured_data, output_filename=\"legal_register_final.pdf\"):\n",
        "    \"\"\"\n",
        "    Génère un PDF à partir d'une structure de données dynamique passée en paramètre.\n",
        "    \"\"\"\n",
        "    # --- Document Setup ---\n",
        "    pagesize = landscape(A3)\n",
        "    margin = 1.0 * inch\n",
        "    doc_width, doc_height = pagesize\n",
        "    table_width = doc_width - 2 * margin\n",
        "\n",
        "    doc = SimpleDocTemplate(\n",
        "        output_filename,\n",
        "        pagesize=pagesize,\n",
        "        rightMargin=margin, leftMargin=margin,\n",
        "        topMargin=1.5 * inch, bottomMargin=margin,\n",
        "    )\n",
        "\n",
        "    # --- Styles ---\n",
        "    styles = getSampleStyleSheet()\n",
        "    header_style = ParagraphStyle(name='HeaderStyle', parent=styles['Normal'], fontName='Helvetica-Bold', fontSize=5, leading=8, alignment=TA_CENTER, textColor=colors.white)\n",
        "    category_header_style = ParagraphStyle(name='CategoryHeaderStyle', parent=header_style, fontSize=6, leading=10, alignment=TA_LEFT) # Alignement à gauche pour les titres\n",
        "    cell_style = ParagraphStyle(name='CellStyle', parent=styles['Normal'], fontName='Helvetica', alignment=TA_LEFT, fontSize=5, leading=7)\n",
        "    bold_cell_style = ParagraphStyle(name='BoldCellStyle', parent=cell_style, fontName='Helvetica-Bold')\n",
        "\n",
        "    # --- Fonctions pour créer les paragraphes ---\n",
        "    def create_header_paragraph(text): return Paragraph(str(text).replace('\\n', '<br/>'), header_style)\n",
        "    def create_category_paragraph(text): return Paragraph(str(text).replace('\\n', '<br/>'), category_header_style)\n",
        "    def create_paragraph(text, style=cell_style): return Paragraph(str(text).replace('\\n', '<br/>'), style)\n",
        "\n",
        "    # --- Assemblage Dynamique de la Table ---\n",
        "    header_row_str = ['Phase', 'Activity/Aspect', 'Impacts', 'Jurisdiction', 'Type', 'Legal Requirement', 'Date', 'Description', 'Task', 'Responsibility', 'Comments']\n",
        "    table_data = [[create_header_paragraph(h) for h in header_row_str]]\n",
        "\n",
        "    red_header_indices = []\n",
        "\n",
        "    # La fonction utilise maintenant les données passées en paramètre\n",
        "    for category in structured_data:\n",
        "        category_title = category.get(\"category_title\", \"Uncategorized\")\n",
        "\n",
        "        red_header_indices.append(len(table_data))\n",
        "        red_header_row = [create_category_paragraph(category_title)] + [''] * (len(header_row_str) - 1)\n",
        "        table_data.append(red_header_row)\n",
        "\n",
        "        for row_data in category.get(\"rows\", []):\n",
        "            processed_row = [\n",
        "                create_paragraph(row_data[0], bold_cell_style),\n",
        "                *[create_paragraph(cell) for cell in row_data[1:]]\n",
        "            ]\n",
        "            table_data.append(processed_row)\n",
        "\n",
        "    # --- Largeurs de Colonnes ---\n",
        "    col_widths_proportions = [0.07, 0.08, 0.07, 0.05, 0.05, 0.15, 0.06, 0.15, 0.12, 0.07, 0.13]\n",
        "    col_widths = [p * table_width for p in col_widths_proportions]\n",
        "\n",
        "    # --- Style de la Table ---\n",
        "    style_commands = [\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
        "        ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n",
        "        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#002060')),\n",
        "        # Padding\n",
        "        ('TOP_PADDING', (0, 0), (-1, -1), 3),\n",
        "        ('BOTTOM_PADDING', (0, 0), (-1, -1), 3),\n",
        "        ('LEFT_PADDING', (0, 0), (-1, -1), 3),\n",
        "        ('RIGHT_PADDING', (0, 0), (-1, -1), 3),\n",
        "    ]\n",
        "    for row_idx in red_header_indices:\n",
        "        style_commands.append(('SPAN', (0, row_idx), (-1, row_idx)))\n",
        "        style_commands.append(('BACKGROUND', (0, row_idx), (-1, row_idx), colors.HexColor('#C00000')))\n",
        "\n",
        "    # --- Création et Sauvegarde du PDF ---\n",
        "    table = Table(table_data, colWidths=col_widths)\n",
        "    table.setStyle(TableStyle(style_commands))\n",
        "\n",
        "    doc.build([table])\n",
        "    print(f\"PDF '{output_filename}' généré avec succès.\")\n",
        "    print(f\"Fichier sauvegardé à : {os.path.abspath(output_filename)}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestre le chargement des données CSV, leur transformation en structure\n",
        "    pour le PDF, puis appelle le générateur de PDF.\n",
        "    \"\"\"\n",
        "    input_csv_path = 'registre_legal_output.csv'\n",
        "\n",
        "    try:\n",
        "        # MODIFICATION 1 : Le séparateur est maintenant une virgule ','\n",
        "        df = pd.read_csv(input_csv_path, sep=',')\n",
        "        df.fillna(\"\", inplace=True)\n",
        "        print(f\"{len(df)} exigences légales chargées depuis '{input_csv_path}'.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERREUR : Le fichier CSV '{input_csv_path}' n'a pas été trouvé.\")\n",
        "        return\n",
        "\n",
        "    # 2. Transformer les données en la structure attendue par la fonction PDF\n",
        "    structured_data_from_llm = []\n",
        "\n",
        "    # Définir l'ordre souhaité des catégories\n",
        "    category_order = [\n",
        "        'General Environmental & Sustainability Regulations',\n",
        "        'Solid Waste & auxilary products',\n",
        "        'Water and Liquid discharges',\n",
        "        'Noise and vibrations',\n",
        "        'Energy',\n",
        "        'Air',\n",
        "        # MODIFICATION 2 : Correction de la faute de frappe pour correspondre au CSV\n",
        "        'Hazardous materials',\n",
        "        'Occupational Health and safety',\n",
        "        'Réglementaire & Permis',\n",
        "    ]\n",
        "\n",
        "    # MODIFICATION 3 : Nous utilisons 'CategoryTitle' pour le regroupement, pas 'Impacts'\n",
        "    grouping_column = 'CategoryTitle'\n",
        "\n",
        "    if grouping_column not in df.columns:\n",
        "        print(f\"ERREUR: La colonne de regroupement '{grouping_column}' n'existe pas dans le CSV.\")\n",
        "        print(f\"Colonnes disponibles : {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # On convertit la colonne de groupement en type Catégorie pour respecter l'ordre\n",
        "    df[grouping_column] = pd.Categorical(df[grouping_column], categories=category_order, ordered=True)\n",
        "    df = df.sort_values(grouping_column)\n",
        "\n",
        "    # MODIFICATION 4 : Regroupement par 'CategoryTitle'\n",
        "    grouped = df.groupby(grouping_column, observed=False)\n",
        "\n",
        "    for category_name, group in grouped:\n",
        "        category_dict = {\n",
        "            \"category_title\": category_name,\n",
        "            \"rows\": []\n",
        "        }\n",
        "\n",
        "        for index, row in group.iterrows():\n",
        "            # Assurez-vous que l'ordre des colonnes ici correspond à 'header_row_str'\n",
        "            row_list = [\n",
        "                row.get('Phase', ''),\n",
        "                row.get('Activity/Aspect', ''),\n",
        "                row.get('Impacts', ''), # La colonne 'Impacts' est toujours affichée, mais pas utilisée pour grouper\n",
        "                row.get('Jurisdiction', ''),\n",
        "                row.get('Type', ''),\n",
        "                row.get('Legal Requirement', ''),\n",
        "                row.get('Date', ''),\n",
        "                row.get('Description', ''),\n",
        "                row.get('Task', ''),\n",
        "                row.get('Responsibility', ''),\n",
        "                row.get('Comments', '')\n",
        "            ]\n",
        "            category_dict[\"rows\"].append(row_list)\n",
        "\n",
        "        # On n'ajoute la catégorie que si elle contient des lignes\n",
        "        if category_dict[\"rows\"]:\n",
        "            structured_data_from_llm.append(category_dict)\n",
        "\n",
        "    print(f\"Données transformées en {len(structured_data_from_llm)} catégories pour le PDF.\")\n",
        "\n",
        "    # 3. Appeler la fonction de génération de PDF avec les données réelles\n",
        "    generate_legal_register_pdf(structured_data_from_llm)\n",
        "\n",
        "# --- Lancement du script ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFApf7wevZXg"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import json\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "import os\n",
        "import time\n",
        "import tempfile\n",
        "import shutil\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.pagesizes import landscape, A3\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.enums import TA_CENTER, TA_LEFT\n",
        "\n",
        "# Global variables for the model\n",
        "text_generator = None\n",
        "model_loaded = False\n",
        "\n",
        "# --- Your existing functions (keeping them exactly as they are) ---\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text: return \"\"\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def get_page_offset(doc, toc_map):\n",
        "    \"\"\"\n",
        "    Calcule l'offset de manière fiable en testant les premiers actes du sommaire.\n",
        "    \"\"\"\n",
        "    if not toc_map:\n",
        "        raise ValueError(\"La table des matières est vide, impossible de calculer l'offset.\")\n",
        "\n",
        "    # On teste les 3 premiers actes du sommaire pour plus de robustesse\n",
        "    for act_to_find in toc_map[:3]:\n",
        "        try:\n",
        "            printed_page_num = act_to_find['start_page_in_doc']\n",
        "\n",
        "            # Utiliser une partie unique du titre pour la recherche\n",
        "            search_snippet = clean_text(act_to_find['legal_requirement'].split('portant')[0])\n",
        "            if len(search_snippet) < 20 or \"n°\" not in search_snippet:\n",
        "                search_snippet = clean_text(act_to_find['legal_requirement'])[:60]\n",
        "\n",
        "            pattern = re.compile(re.escape(search_snippet), re.IGNORECASE)\n",
        "\n",
        "            for i, page in enumerate(doc):\n",
        "                page_text = clean_text(page.get_text(\"text\"))\n",
        "                if pattern.search(page_text):\n",
        "                    # Si on trouve, on a notre offset et on quitte la fonction\n",
        "                    return printed_page_num - i\n",
        "        except Exception:\n",
        "            # Si un acte échoue, on continue simplement avec le suivant\n",
        "            continue\n",
        "\n",
        "    raise ValueError(f\"Impossible de localiser les premiers actes du sommaire dans le document pour calculer l'offset.\")\n",
        "\n",
        "def map_table_of_contents_simplified(doc):\n",
        "    \"\"\"\n",
        "    Scanne le texte des premières pages pour trouver les entrées du sommaire.\n",
        "    \"\"\"\n",
        "    toc_map = []\n",
        "    full_toc_text = \"\"\n",
        "    for i in range(min(2, len(doc))):\n",
        "        full_toc_text += doc[i].get_text(\"text\")\n",
        "\n",
        "    full_toc_text = clean_text(full_toc_text)\n",
        "\n",
        "    # Regex amélioré pour capturer les titres, même s'ils contiennent eux-mêmes des points\n",
        "    pattern = re.compile(\n",
        "        r\"((?:Arrêté|Décision|Décret|Loi|Dahir|Avis)\\s+.*?n°\\s*[\\w\\.\\-]+.*?)\\s*\\.{5,}\\s*(\\d{3,})\",\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    matches = pattern.findall(full_toc_text)\n",
        "\n",
        "    for match in matches:\n",
        "        title = clean_text(match[0])\n",
        "        page_number = int(match[1])\n",
        "        category_match = re.search(r\"^(Arrêté|Décision|Décret|Loi|Dahir|Avis)\", title, re.IGNORECASE)\n",
        "        category = category_match.group(1).capitalize() if category_match else \"Inconnu\"\n",
        "        toc_map.append({\n",
        "            \"legal_requirement\": title,\n",
        "            \"category\": category,\n",
        "            \"start_page_in_doc\": page_number,\n",
        "        })\n",
        "\n",
        "    toc_map.sort(key=lambda x: x['start_page_in_doc'])\n",
        "\n",
        "    unique_map = []\n",
        "    seen_titles = set()\n",
        "    for item in toc_map:\n",
        "        if item['legal_requirement'] not in seen_titles:\n",
        "            unique_map.append(item)\n",
        "            seen_titles.add(item['legal_requirement'])\n",
        "\n",
        "    return unique_map\n",
        "\n",
        "def extract_full_content_from_map(doc, toc_map, page_offset):\n",
        "    \"\"\"Extrait le contenu complet en se basant sur la carte du sommaire.\"\"\"\n",
        "    final_acts = []\n",
        "    num_pages_in_doc = len(doc)\n",
        "\n",
        "    for i, act in enumerate(toc_map):\n",
        "        start_index = act['start_page_in_doc'] - page_offset\n",
        "\n",
        "        if i + 1 < len(toc_map):\n",
        "            next_act_start_page = toc_map[i + 1]['start_page_in_doc']\n",
        "            end_index = next_act_start_page - page_offset\n",
        "        else:\n",
        "            end_index = num_pages_in_doc\n",
        "\n",
        "        start_index = max(0, start_index)\n",
        "        end_index = min(num_pages_in_doc, end_index)\n",
        "\n",
        "        content_parts = []\n",
        "        if start_index < end_index:\n",
        "            for page_num in range(start_index, end_index):\n",
        "                content_parts.append(doc[page_num].get_text(\"text\", sort=True))\n",
        "        elif 0 <= start_index < num_pages_in_doc:\n",
        "             content_parts.append(doc[start_index].get_text(\"text\", sort=True))\n",
        "\n",
        "        full_content = \" \".join(content_parts)\n",
        "        cleaned_content = clean_text(full_content)\n",
        "\n",
        "        act['full_text_content'] = cleaned_content\n",
        "        final_acts.append(act)\n",
        "\n",
        "    return final_acts\n",
        "\n",
        "def call_llm_mistral(prompt):\n",
        "    \"\"\"\n",
        "    Formate le prompt pour Mistral Instruct, appelle le pipeline local,\n",
        "    et nettoie la réponse pour extraire le JSON.\n",
        "    \"\"\"\n",
        "    global text_generator\n",
        "    if text_generator is None:\n",
        "        return {\"error\": \"Model not loaded\"}\n",
        "\n",
        "    # Template correct pour Mistral Instruct\n",
        "    formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
        "\n",
        "    try:\n",
        "        output = text_generator(formatted_prompt)\n",
        "        raw_response = output[0]['generated_text']\n",
        "        # Extraire uniquement le texte généré par le modèle après le prompt\n",
        "        cleaned_response = raw_response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "        # Tentative d'extraire le JSON du texte\n",
        "        # Le modèle peut parfois ajouter des commentaires avant/après le JSON\n",
        "        json_match = re.search(r'\\{.*\\}', cleaned_response, re.DOTALL)\n",
        "        if json_match:\n",
        "            return json.loads(json_match.group(0))\n",
        "        else:\n",
        "            print(f\"  -> Avertissement : Le LLM n'a pas retourné un JSON valide. Réponse brute : {cleaned_response}\")\n",
        "            return None # Retourne None en cas d'échec\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"  -> Erreur : Impossible de décoder le JSON de la réponse du LLM. Réponse brute : {cleaned_response}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"  -> Erreur lors de l'appel LLM : {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_legal_act(act_data, project_description):\n",
        "    \"\"\"\n",
        "    Analyse un acte juridique avec un prompt CoT finalisé qui insiste\n",
        "    lourdement sur le format de sortie JSON.\n",
        "    \"\"\"\n",
        "    print(f\"\\n  -> Analyse de : {textwrap.shorten(act_data['legal_requirement'], 100)}\")\n",
        "\n",
        "    # --- PROMPT FINAL AVEC INSTRUCTION DE FORMATAGE RENFORCÉE ---\n",
        "    prompt = f\"\"\"\n",
        "    Tu es un assistant expert en conformité réglementaire. Ta tâche est d'analyser un texte de loi et de le classer pour un projet industriel.\n",
        "\n",
        "    **Contexte du Projet:**\n",
        "    {project_description}\n",
        "\n",
        "    **Texte de Loi à Analyser:**\n",
        "    - Titre : {act_data['legal_requirement']}\n",
        "    - Contenu :\n",
        "    \\\"\\\"\\\"\n",
        "    {textwrap.shorten(act_data['full_text_content'], 4000, placeholder='...')}\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    **Instructions:**\n",
        "    Suis ce processus de raisonnement pour arriver à ta réponse finale :\n",
        "    1.  **Résumé et Pertinence :** D'abord, résume le texte. Ensuite, détermine si le sujet a un impact DIRECT sur une usine chimique (construction, opération, sécurité, environnement, permis).\n",
        "    2.  **Classification :** Si le texte est pertinent, choisis la catégorie la plus appropriée dans cette liste : ['General Environmental & Sustainability Regulations', 'Solid Waste & auxilary products', 'Water and Liquid discharges', 'Noise and vibrations', 'Energy', 'Air', 'Hazardeous materials', 'Occupational Health and safety']. Si non pertinent, la catégorie est \"Non Applicable\".\n",
        "    3.  **Remplissage des Champs :** Remplis tous les champs requis en te basant sur ton analyse. Pour les champs avec des choix contraints, utilise UNIQUEMENT les options fournies.\n",
        "\n",
        "    **Réponse Finale:**\n",
        "    Après avoir suivi ces étapes de raisonnement, fournis ta réponse finale. Ta réponse doit être **uniquement et exclusivement un dictionnaire JSON valide**. N'ajoute aucun texte, aucune explication, ni de numérotation avant ou après le JSON.\n",
        "\n",
        "    **Format JSON Obligatoire:**\n",
        "    {{\n",
        "      \"category_title\": \"...\",\n",
        "      \"description\": \"...\",\n",
        "      \"phase\": \"['All project phases', 'Pre-construction', 'Evaluate/Define', 'Construction/Operation']\",\n",
        "      \"activity_aspect\": \"...\",\n",
        "      \"impacts\": \"['Environment', 'Health & Safety', 'Regulatory & Permitting']\",\n",
        "      \"type\": \"['Regulation', 'Guideline', 'Standard']\",\n",
        "      \"task\": \"...\",\n",
        "      \"responsibility\": \"...\",\n",
        "      \"comments\": \"...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # On utilise votre fonction existante pour appeler le modèle\n",
        "    analysis_result = call_llm_mistral(prompt)\n",
        "    return analysis_result\n",
        "\n",
        "def generate_legal_register_pdf(structured_data, output_filename=\"legal_register_final.pdf\"):\n",
        "    \"\"\"\n",
        "    Génère un PDF à partir d'une structure de données dynamique passée en paramètre.\n",
        "    \"\"\"\n",
        "    # --- Document Setup ---\n",
        "    pagesize = landscape(A3)\n",
        "    margin = 1.0 * inch\n",
        "    doc_width, doc_height = pagesize\n",
        "    table_width = doc_width - 2 * margin\n",
        "\n",
        "    doc = SimpleDocTemplate(\n",
        "        output_filename,\n",
        "        pagesize=pagesize,\n",
        "        rightMargin=margin, leftMargin=margin,\n",
        "        topMargin=1.5 * inch, bottomMargin=margin,\n",
        "    )\n",
        "\n",
        "    # --- Styles ---\n",
        "    styles = getSampleStyleSheet()\n",
        "    header_style = ParagraphStyle(name='HeaderStyle', parent=styles['Normal'], fontName='Helvetica-Bold', fontSize=5, leading=8, alignment=TA_CENTER, textColor=colors.white)\n",
        "    category_header_style = ParagraphStyle(name='CategoryHeaderStyle', parent=header_style, fontSize=6, leading=10, alignment=TA_LEFT) # Alignement à gauche pour les titres\n",
        "    cell_style = ParagraphStyle(name='CellStyle', parent=styles['Normal'], fontName='Helvetica', alignment=TA_LEFT, fontSize=5, leading=7)\n",
        "    bold_cell_style = ParagraphStyle(name='BoldCellStyle', parent=cell_style, fontName='Helvetica-Bold')\n",
        "\n",
        "    # --- Fonctions pour créer les paragraphes ---\n",
        "    def create_header_paragraph(text): return Paragraph(str(text).replace('\\n', '<br/>'), header_style)\n",
        "    def create_category_paragraph(text): return Paragraph(str(text).replace('\\n', '<br/>'), category_header_style)\n",
        "    def create_paragraph(text, style=cell_style): return Paragraph(str(text).replace('\\n', '<br/>'), style)\n",
        "\n",
        "    # --- Assemblage Dynamique de la Table ---\n",
        "    header_row_str = ['Phase', 'Activity/Aspect', 'Impacts', 'Jurisdiction', 'Type', 'Legal Requirement', 'Date', 'Description', 'Task', 'Responsibility', 'Comments']\n",
        "    table_data = [[create_header_paragraph(h) for h in header_row_str]]\n",
        "\n",
        "    red_header_indices = []\n",
        "\n",
        "    # La fonction utilise maintenant les données passées en paramètre\n",
        "    for category in structured_data:\n",
        "        category_title = category.get(\"category_title\", \"Uncategorized\")\n",
        "\n",
        "        red_header_indices.append(len(table_data))\n",
        "        red_header_row = [create_category_paragraph(category_title)] + [''] * (len(header_row_str) - 1)\n",
        "        table_data.append(red_header_row)\n",
        "\n",
        "        for row_data in category.get(\"rows\", []):\n",
        "            processed_row = [\n",
        "                create_paragraph(row_data[0], bold_cell_style),\n",
        "                *[create_paragraph(cell) for cell in row_data[1:]]\n",
        "            ]\n",
        "            table_data.append(processed_row)\n",
        "\n",
        "    # --- Largeurs de Colonnes ---\n",
        "    col_widths_proportions = [0.07, 0.08, 0.07, 0.05, 0.05, 0.15, 0.06, 0.15, 0.12, 0.07, 0.13]\n",
        "    col_widths = [p * table_width for p in col_widths_proportions]\n",
        "\n",
        "    # --- Style de la Table ---\n",
        "    style_commands = [\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
        "        ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n",
        "        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#002060')),\n",
        "        # Padding\n",
        "        ('TOP_PADDING', (0, 0), (-1, -1), 3),\n",
        "        ('BOTTOM_PADDING', (0, 0), (-1, -1), 3),\n",
        "        ('LEFT_PADDING', (0, 0), (-1, -1), 3),\n",
        "        ('RIGHT_PADDING', (0, 0), (-1, -1), 3),\n",
        "    ]\n",
        "    for row_idx in red_header_indices:\n",
        "        style_commands.append(('SPAN', (0, row_idx), (-1, row_idx)))\n",
        "        style_commands.append(('BACKGROUND', (0, row_idx), (-1, row_idx), colors.HexColor('#C00000')))\n",
        "\n",
        "    # --- Création et Sauvegarde du PDF ---\n",
        "    table = Table(table_data, colWidths=col_widths)\n",
        "    table.setStyle(TableStyle(style_commands))\n",
        "\n",
        "    doc.build([table])\n",
        "    print(f\"PDF '{output_filename}' généré avec succès.\")\n",
        "    print(f\"Fichier sauvegardé à : {os.path.abspath(output_filename)}\")\n",
        "\n",
        "# --- Gradio Functions ---\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load the Mistral model\"\"\"\n",
        "    global text_generator, model_loaded\n",
        "\n",
        "    if model_loaded:\n",
        "        return \"✅ Model already loaded!\"\n",
        "\n",
        "    try:\n",
        "        model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        text_generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.1,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        model_loaded = True\n",
        "        return \"✅ Model loaded successfully!\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error loading model: {str(e)}\"\n",
        "\n",
        "def process_pdf(pdf_file, project_description, progress=gr.Progress()):\n",
        "    \"\"\"Main processing function for Gradio\"\"\"\n",
        "    global text_generator\n",
        "\n",
        "    if not model_loaded or text_generator is None:\n",
        "        return None, None, \"❌ Please load the model first!\"\n",
        "\n",
        "    if pdf_file is None:\n",
        "        return None, None, \"❌ Please upload a PDF file!\"\n",
        "\n",
        "    if not project_description.strip():\n",
        "        project_description = \"\"\"\n",
        "        Projet de construction d'une usine d'additifs chimiques pour le client NOVADDIX.\n",
        "        L'usine produira 20 000 tonnes/an d'additifs.\n",
        "        Domaine : Industriel - Chimie. Classe de produit : Produits inflammables et toxiques.\n",
        "        \"\"\"\n",
        "\n",
        "    try:\n",
        "        progress(0.1, desc=\"Opening PDF...\")\n",
        "\n",
        "        # Create temporary directory for processing\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Copy uploaded file to temp directory\n",
        "        temp_pdf_path = os.path.join(temp_dir, \"uploaded_file.pdf\")\n",
        "        shutil.copy(pdf_file.name, temp_pdf_path)\n",
        "\n",
        "        # Process PDF\n",
        "        doc = fitz.open(temp_pdf_path)\n",
        "\n",
        "        progress(0.2, desc=\"Extracting table of contents...\")\n",
        "\n",
        "        table_of_contents = map_table_of_contents_simplified(doc)\n",
        "\n",
        "        if not table_of_contents:\n",
        "            return None, None, \"❌ Could not parse table of contents from the PDF.\"\n",
        "\n",
        "        progress(0.3, desc=\"Calculating page offset...\")\n",
        "\n",
        "        page_offset = get_page_offset(doc, table_of_contents)\n",
        "\n",
        "        progress(0.4, desc=\"Extracting content...\")\n",
        "\n",
        "        final_data = extract_full_content_from_map(doc, table_of_contents, page_offset)\n",
        "\n",
        "        # Save preprocessed data\n",
        "        preprocessed_path = os.path.join(temp_dir, 'data_preprocessed_BULLETINS.json')\n",
        "        all_bulletins_data = {os.path.basename(temp_pdf_path): final_data}\n",
        "\n",
        "        with open(preprocessed_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_bulletins_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Analyze with LLM\n",
        "        all_analyzed_acts = []\n",
        "        total_acts = len(final_data)\n",
        "\n",
        "        for i, act in enumerate(final_data):\n",
        "            progress(0.5 + (i / total_acts) * 0.4, desc=f\"Analyzing act {i+1}/{total_acts}...\")\n",
        "\n",
        "            llm_analysis = analyze_legal_act(act, project_description)\n",
        "\n",
        "            if llm_analysis:\n",
        "                full_entry = {\n",
        "                    \"Legal Requirement\": act['legal_requirement'],\n",
        "                    \"Date\": re.search(r'\\((\\d{1,2}\\s+\\w+\\s+\\d{4})\\)', act['legal_requirement']).group(1) if re.search(r'\\((\\d{1,2}\\s+\\w+\\s+\\d{4})\\)', act['legal_requirement']) else 'N/A',\n",
        "                    \"Type\": act['category'],\n",
        "                    \"Jurisdiction\": \"National\",\n",
        "                    \"CategoryTitle\": llm_analysis.get(\"category_title\", \"Uncategorized\"),\n",
        "                    \"Description\": llm_analysis.get(\"description\", \"N/A\"),\n",
        "                    \"Phase\": llm_analysis.get(\"phase\", \"N/A\"),\n",
        "                    \"Activity/Aspect\": llm_analysis.get(\"activity_aspect\", \"N/A\"),\n",
        "                    \"Impacts\": llm_analysis.get(\"impacts\", \"N/A\"),\n",
        "                    \"Task\": llm_analysis.get(\"task\", \"N/A\"),\n",
        "                    \"Responsibility\": llm_analysis.get(\"responsibility\", \"N/A\"),\n",
        "                    \"Comments\": llm_analysis.get(\"comments\", \"N/A\"),\n",
        "                }\n",
        "                all_analyzed_acts.append(full_entry)\n",
        "\n",
        "            time.sleep(0.5)  # Small delay to avoid overwhelming the model\n",
        "\n",
        "        progress(0.9, desc=\"Generating outputs...\")\n",
        "\n",
        "        # Create CSV\n",
        "        if all_analyzed_acts:\n",
        "            df = pd.DataFrame(all_analyzed_acts)\n",
        "            csv_path = os.path.join(temp_dir, 'registre_legal_output.csv')\n",
        "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "            # Prepare data for PDF\n",
        "            structured_data_for_pdf = []\n",
        "\n",
        "            # Define category order\n",
        "            category_order = [\n",
        "                'General Environmental & Sustainability Regulations',\n",
        "                'Solid Waste & auxilary products',\n",
        "                'Water and Liquid discharges',\n",
        "                'Noise and vibrations',\n",
        "                'Energy',\n",
        "                'Air',\n",
        "                'Hazardous materials',\n",
        "                'Occupational Health and safety',\n",
        "            ]\n",
        "\n",
        "            # Group by CategoryTitle\n",
        "            df['CategoryTitle'] = pd.Categorical(df['CategoryTitle'], categories=category_order, ordered=True)\n",
        "            df = df.sort_values('CategoryTitle')\n",
        "            grouped = df.groupby('CategoryTitle', observed=False)\n",
        "\n",
        "            for category_name, group in grouped:\n",
        "                if len(group) > 0:  # Only add if there are rows\n",
        "                    category_dict = {\n",
        "                        \"category_title\": category_name,\n",
        "                        \"rows\": []\n",
        "                    }\n",
        "\n",
        "                    for index, row in group.iterrows():\n",
        "                        row_list = [\n",
        "                            row.get('Phase', ''),\n",
        "                            row.get('Activity/Aspect', ''),\n",
        "                            row.get('Impacts', ''),\n",
        "                            row.get('Jurisdiction', ''),\n",
        "                            row.get('Type', ''),\n",
        "                            row.get('Legal Requirement', ''),\n",
        "                            row.get('Date', ''),\n",
        "                            row.get('Description', ''),\n",
        "                            row.get('Task', ''),\n",
        "                            row.get('Responsibility', ''),\n",
        "                            row.get('Comments', '')\n",
        "                        ]\n",
        "                        category_dict[\"rows\"].append(row_list)\n",
        "\n",
        "                    structured_data_for_pdf.append(category_dict)\n",
        "\n",
        "            # Generate PDF\n",
        "            pdf_path = os.path.join(temp_dir, 'legal_register_final.pdf')\n",
        "            generate_legal_register_pdf(structured_data_for_pdf, pdf_path)\n",
        "\n",
        "            progress(1.0, desc=\"Complete!\")\n",
        "\n",
        "            # Copy files to a location where Gradio can access them\n",
        "            final_csv_path = os.path.join(os.getcwd(), f\"output_{int(time.time())}_registre_legal.csv\")\n",
        "            final_pdf_path = os.path.join(os.getcwd(), f\"output_{int(time.time())}_legal_register.pdf\")\n",
        "\n",
        "            shutil.copy(csv_path, final_csv_path)\n",
        "            shutil.copy(pdf_path, final_pdf_path)\n",
        "\n",
        "            success_message = f\"✅ Processing complete!\\n- {len(table_of_contents)} acts found in table of contents\\n- {len(all_analyzed_acts)} acts successfully analyzed\\n- Files generated: CSV and PDF\"\n",
        "\n",
        "            # Cleanup\n",
        "            doc.close()\n",
        "            shutil.rmtree(temp_dir)\n",
        "\n",
        "            return final_csv_path, final_pdf_path, success_message\n",
        "        else:\n",
        "            return None, None, \"❌ No acts were successfully analyzed.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'doc' in locals():\n",
        "            doc.close()\n",
        "        if 'temp_dir' in locals():\n",
        "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "        return None, None, f\"❌ Error processing PDF: {str(e)}\"\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Legal Register PDF Processor\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# 🏛️ Legal Register PDF Processor\")\n",
        "        gr.Markdown(\"Upload a PDF bulletin and generate a legal compliance register with AI analysis.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"## Step 1: Load Model\")\n",
        "                load_btn = gr.Button(\"🤖 Load Mistral Model\", variant=\"primary\")\n",
        "                model_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "                gr.Markdown(\"## Step 2: Upload PDF & Configure\")\n",
        "                pdf_input = gr.File(\n",
        "                    label=\"Upload PDF Bulletin\",\n",
        "                    file_types=[\".pdf\"],\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                project_desc = gr.Textbox(\n",
        "                    label=\"Project Description\",\n",
        "                    placeholder=\"Describe your industrial project...\",\n",
        "                    lines=4,\n",
        "                    value=\"\"\"Projet de construction d'une usine d'additifs chimiques pour le client NOVADDIX.\n",
        "L'usine produira 20 000 tonnes/an d'additifs.\n",
        "Domaine : Industriel - Chimie. Classe de produit : Produits inflammables et toxiques.\"\"\"\n",
        "                )\n",
        "\n",
        "                process_btn = gr.Button(\"🚀 Process PDF\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"## Results\")\n",
        "                status_output = gr.Textbox(label=\"Processing Status\", lines=3)\n",
        "\n",
        "                with gr.Row():\n",
        "                    csv_download = gr.File(label=\"📊 Download CSV\")\n",
        "                    pdf_download = gr.File(label=\"📋 Download PDF Report\")\n",
        "\n",
        "        # Event handlers\n",
        "        load_btn.click(\n",
        "            fn=load_model,\n",
        "            outputs=[model_status]\n",
        "        )\n",
        "\n",
        "        process_btn.click(\n",
        "            fn=process_pdf,\n",
        "            inputs=[pdf_input, project_desc],\n",
        "            outputs=[csv_download, pdf_download, status_output]\n",
        "        )\n",
        "\n",
        "        # Examples section\n",
        "        gr.Markdown(\"## 📖 Instructions\")\n",
        "        gr.Markdown(\"\"\"\n",
        "        1. **Load the Model**: Click the \"Load Mistral Model\" button and wait for confirmation\n",
        "        2. **Upload PDF**: Select a legal bulletin PDF file\n",
        "        3. **Configure Project**: Edit the project description if needed\n",
        "        4. **Process**: Click \"Process PDF\" and wait for the analysis to complete\n",
        "        5. **Download**: Get your CSV data and PDF report\n",
        "\n",
        "        **Note**: The first model load may take several minutes. Processing time depends on the number of legal acts in your PDF.\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}