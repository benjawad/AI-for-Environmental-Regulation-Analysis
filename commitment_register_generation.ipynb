{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPb9XJNakX4kJu7U7sbtkYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjawad/AI-for-Environmental-Regulation-Analysis/blob/main/commitment_register_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai tqdm reportlab PyPDF2 gradio pymupdf\n",
        "!pip install pytesseract Pillow arabic_reshaper python-bidi\n",
        "!pip install selenium webdriver-manager beautifulsoup4 requests playwright  chromium\n",
        "!playwright install > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mp7VnrPo2oa",
        "outputId": "3e420383-6ea8-44f2-8e35-16c0b1f8050e"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.11/dist-packages (4.4.3)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.41.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab) (3.4.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: arabic_reshaper in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (0.6.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.35.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting playwright\n",
            "  Downloading playwright-1.54.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting chromium\n",
            "  Downloading chromium-0.0.0-py3-none-any.whl.metadata (615 bytes)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.3)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading playwright-1.54.0-py3-none-manylinux1_x86_64.whl (45.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.9/45.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromium-0.0.0-py3-none-any.whl (2.4 kB)\n",
            "Downloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, chromium, playwright\n",
            "Successfully installed chromium-0.0.0 playwright-1.54.0 pyee-13.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# web Scrapping"
      ],
      "metadata": {
        "id": "sjNdpka3LHn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "BASE_URL = \"https://environnement.gov.ma/fr/lois-et-reglementations/normes\"\n",
        "pdf_links = set()\n",
        "MAX_PAGES = 4\n",
        "\n",
        "async def extract_pdfs_from_html(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    new_links = set()\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if href.lower().endswith(\".pdf\"):\n",
        "            full_link = urljoin(BASE_URL, href)\n",
        "            new_links.add(full_link)\n",
        "\n",
        "    return new_links\n",
        "\n",
        "async def run():\n",
        "    async with async_playwright() as p:\n",
        "        # Lancement du navigateur avec user-agent personnalis√©\n",
        "        browser = await p.chromium.launch(headless=True)  # Mettre True en production\n",
        "        context = await browser.new_context(user_agent=(\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "            \"(KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
        "        ))\n",
        "        page = await context.new_page()\n",
        "\n",
        "        print(f\"üîó Acc√®s √† la page : {BASE_URL}\")\n",
        "        await page.goto(BASE_URL)\n",
        "        await page.wait_for_load_state(\"networkidle\")\n",
        "        await asyncio.sleep(5)  # Laisser le temps au contenu dynamique\n",
        "\n",
        "        # --- Option 1: Extraire les liens directement depuis la page principale ---\n",
        "        print(\"üîç Extraction depuis la page principale...\")\n",
        "        main_html = await page.content()\n",
        "        new_links = await extract_pdfs_from_html(main_html)\n",
        "        pdf_links.update(new_links)\n",
        "\n",
        "        print(f\"üìÑ {len(pdf_links)} liens PDF trouv√©s dans la page principale\")\n",
        "\n",
        "        # --- Option 2: Exploration des iframes pour trouver des PDF suppl√©mentaires ---\n",
        "        frames = page.frames\n",
        "        print(f\"üß© {len(frames)} frames d√©tect√©es\")\n",
        "\n",
        "        for frame in frames:\n",
        "            print(\"   - Frame URL:\", frame.url)\n",
        "            try:\n",
        "                frame_html = await frame.content()\n",
        "                frame_links = await extract_pdfs_from_html(frame_html)\n",
        "                count = len(frame_links - pdf_links)\n",
        "                pdf_links.update(frame_links)\n",
        "                print(f\"   ‚úÖ {count} nouveaux PDFs trouv√©s dans cette frame\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Impossible de lire la frame : {e}\")\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "        print(f\"\\n‚úÖ Scraping termin√©. Total de liens PDF trouv√©s : {len(pdf_links)}\")\n",
        "        for link in sorted(pdf_links):\n",
        "            print(link)\n",
        "\n",
        "# Ex√©cution\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "await run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "somjUzCiLHKP",
        "outputId": "343a994b-1112-4d43-857b-06f306ea0465"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Acc√®s √† la page : https://environnement.gov.ma/fr/lois-et-reglementations/normes\n",
            "üîç Extraction depuis la page principale...\n",
            "üìÑ 17 liens PDF trouv√©s dans la page principale\n",
            "üß© 1 frames d√©tect√©es\n",
            "   - Frame URL: https://environnement.gov.ma/fr/lois-et-reglementations/normes\n",
            "   ‚úÖ 0 nouveaux PDFs trouv√©s dans cette frame\n",
            "\n",
            "‚úÖ Scraping termin√©. Total de liens PDF trouv√©s : 17\n",
            "https://environnement.gov.ma/PDFs/LETTRE_ROYALE.pdf\n",
            "https://environnement.gov.ma/PDFs/decretCNE.pdf\n",
            "https://environnement.gov.ma/images/Normes/Air/Normes_de_la_qualit√©_de_lair.pdf\n",
            "https://environnement.gov.ma/images/Normes/Air/Seuils_dinformation_et_seuils_dalerte.pdf\n",
            "https://environnement.gov.ma/images/Normes/Air/Valeurs_limites_g√©n√©rales_des_rejets_atmosph√©riques.pdf\n",
            "https://environnement.gov.ma/images/Normes/Air/Valeurs_limites_sectorielles__c√©ramique.pdf\n",
            "https://environnement.gov.ma/images/Normes/Air/valeurs_limites_sp√©cifiques_du_secteur_cimentier.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/Normes_qualit√©__des_eaux_us√©es_√©pur√©es_detinees_a_l_irrigation.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLG_2018_des_rejets_industriels__liquides.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS__de_traitement_du_surface_1.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS__du_secteur_c√©ramique.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS_de_lindustrie_du_sucre.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS_des_rejets_domestiques.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS_du__papier_cartons.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS_du_secteur_cimentier.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS_du_secteur_peinture_et_vernis.pdf\n",
            "https://environnement.gov.ma/images/Normes/Eau/VLS_du_secteur_textile.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import requests\n",
        "\n",
        "os.makedirs(\"/content/pdfs\", exist_ok=True)\n",
        "pdf_texts = []\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115 Safari/537.36\"\n",
        "}\n",
        "\n",
        "for url in pdf_links:\n",
        "    filename = os.path.basename(url)\n",
        "    path = os.path.join(\"/content/pdfs\", filename)\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        content_type = r.headers.get(\"Content-Type\", \"\").lower()\n",
        "        if r.status_code == 200 and \"application/pdf\" in content_type:\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "\n",
        "            # Essayer d'ouvrir le fichier PDF avec fitz\n",
        "            try:\n",
        "                doc = fitz.open(path)\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "                pdf_texts.append((filename, text[:1000]))  # Enregistre un extrait\n",
        "            except Exception as e:\n",
        "                print(f\"üóëÔ∏è PDF corrompu (lecture impossible): {filename} -> supprim√©\")\n",
        "                os.remove(path)\n",
        "        else:\n",
        "            print(f\"‚ùå Mauvais type de contenu pour {filename} ({content_type})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur pour {filename}: {e}\")\n",
        "\n",
        "# Afficher un √©chantillon des textes extraits\n",
        "for name, content in pdf_texts[:2]:\n",
        "    print(f\"\\nüìÑ {name}\\n\")\n",
        "pdf_names = []\n",
        "for name, content in pdf_texts:\n",
        "    pdf_names.append(\"/pdfs/\"+name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UejqRcmMMZL8",
        "outputId": "95dd056f-4e4b-4702-aacd-a12cd11ed5e6"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Valeurs_limites_sectorielles__c√©ramique.pdf\n",
            "\n",
            "\n",
            "üìÑ Normes_de_la_qualit√©_de_lair.pdf\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pdf Parsing"
      ],
      "metadata": {
        "id": "xqjShXCHLfhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import traceback\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import logging\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class TableValidationConfig:\n",
        "    \"\"\"Configuration pour la validation des tableaux\"\"\"\n",
        "    max_columns: int = 12\n",
        "    min_rows: int = 2\n",
        "    max_null_percentage: float = 0.4\n",
        "    min_content_ratio: float = 0.3\n",
        "\n",
        "class RobustPDFParser:\n",
        "    \"\"\"\n",
        "    Parser PDF de production robuste qui adresse les probl√®mes identifi√©s :\n",
        "    - Strat√©gie multi-approche pour les diff√©rents types de documents\n",
        "    - Gestion robuste des mises en page complexes\n",
        "    - OCR int√©gr√© avec d√©tection automatique\n",
        "    - Validation stricte des tableaux pour √©viter les hallucinations\n",
        "    - Classification intelligente des documents\n",
        "    \"\"\"\n",
        "\n",
        "    # Configurations pour la d√©tection des polluants (enrichie)\n",
        "    POLLUTANT_MAPPING = {\n",
        "        'SO2': ['dioxyde de soufre', 'so2', 'sulphur dioxide', 'anhydride sulfureux'],\n",
        "        'NOx': ['oxydes d\\'azote', 'nox', 'nitrogen oxides', 'monoxyde d\\'azote', 'dioxyde d\\'azote'],\n",
        "        'COV': ['compos√©s organiques volatils', 'cov', 'volatile organic compounds', 'voc'],\n",
        "        'PM10': ['particules pm10', 'pm10', 'particulate matter 10', 'poussi√®res pm10'],\n",
        "        'PM2.5': ['particules pm2.5', 'pm2.5', 'particules fines'],\n",
        "        'Hg': ['mercure', 'mercury', 'hg'],\n",
        "        'Pb': ['plomb', 'lead', 'pb'],\n",
        "        'Cd': ['cadmium', 'cd'],\n",
        "        'O3': ['ozone', 'o3'],\n",
        "        'CO': ['monoxyde de carbone', 'co', 'carbon monoxide'],\n",
        "        'Benz√®ne': ['benz√®ne', 'benzene', 'c6h6'],\n",
        "        'H2S': ['sulfure d\\'hydrog√®ne', 'h2s', 'hydrogen sulfide'],\n",
        "        'NH3': ['ammoniac', 'nh3', 'ammonia'],\n",
        "        'Fluorures': ['fluorures', 'fluorides', 'hf'],\n",
        "        'Chlorures': ['chlorures', 'chlorides', 'hcl']\n",
        "    }\n",
        "\n",
        "    def __init__(self, pdf_path: str, config: Optional[TableValidationConfig] = None):\n",
        "        if not os.path.exists(pdf_path):\n",
        "            raise FileNotFoundError(f\"Fichier introuvable: {pdf_path}\")\n",
        "\n",
        "        self.pdf_path = pdf_path\n",
        "        self.filename = os.path.basename(pdf_path)\n",
        "        self.doc = fitz.open(pdf_path)\n",
        "        self.config = config or TableValidationConfig()\n",
        "\n",
        "        # Analyse pr√©liminaire du document\n",
        "        self.doc_analysis = self._analyze_document_structure()\n",
        "        self.doc_type = self._classify_document()\n",
        "        self.metadata = self._extract_metadata()\n",
        "\n",
        "    def _analyze_document_structure(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyse la structure g√©n√©rale du document\"\"\"\n",
        "        analysis = {\n",
        "            'total_pages': len(self.doc),\n",
        "            'scanned_pages': 0,\n",
        "            'text_pages': 0,\n",
        "            'table_pages': 0,\n",
        "            'mixed_pages': 0,\n",
        "            'avg_text_density': 0,\n",
        "            'has_images': False\n",
        "        }\n",
        "\n",
        "        text_densities = []\n",
        "\n",
        "        for page_num in range(min(5, len(self.doc))):  # Analyse des 5 premi√®res pages\n",
        "            page = self.doc.load_page(page_num)\n",
        "\n",
        "            # Analyse du texte\n",
        "            text = page.get_text()\n",
        "            text_density = len(text.strip()) / (page.rect.width * page.rect.height) * 10000\n",
        "            text_densities.append(text_density)\n",
        "\n",
        "            # D√©tection du type de page\n",
        "            if self._is_scanned_page(page):\n",
        "                analysis['scanned_pages'] += 1\n",
        "            elif self._has_complex_tables(page):\n",
        "                analysis['table_pages'] += 1\n",
        "            elif text_density > 5:\n",
        "                analysis['text_pages'] += 1\n",
        "            else:\n",
        "                analysis['mixed_pages'] += 1\n",
        "\n",
        "            # D√©tection d'images\n",
        "            if page.get_images():\n",
        "                analysis['has_images'] = True\n",
        "\n",
        "        analysis['avg_text_density'] = np.mean(text_densities) if text_densities else 0\n",
        "        return analysis\n",
        "\n",
        "    def _classify_document(self) -> str:\n",
        "        \"\"\"Classification intelligente bas√©e sur le nom et le contenu\"\"\"\n",
        "        filename_lower = self.filename.lower()\n",
        "\n",
        "        # Classification par nom de fichier\n",
        "        filename_patterns = [\n",
        "            (r'valeurs?.*limites?.*g√©n√©rales?.*atmosph√©rique', 'vlg_atmospherique'),\n",
        "            (r'valeurs?.*limites?.*g√©n√©rales?.*liquide', 'vlg_liquide'),\n",
        "            (r'valeurs?.*limites?.*sectorielles?.*ciment', 'vls_ciment'),\n",
        "            (r'valeurs?.*limites?.*sectorielles?.*c√©ramique', 'vls_ceramique'),\n",
        "            (r'valeurs?.*limites?.*sectorielles?', 'vls_autre'),\n",
        "            (r'normes?.*qualit√©.*air', 'normes_air'),\n",
        "            (r'normes?.*qualit√©.*eau', 'normes_eau'),\n",
        "            (r'seuils?.*information.*alerte', 'seuils'),\n",
        "            (r'd√©cret|decret', 'decret'),\n",
        "            (r'lettre.*royale', 'lettre_royale'),\n",
        "            (r'irrigation', 'irrigation')\n",
        "        ]\n",
        "\n",
        "        for pattern, doc_type in filename_patterns:\n",
        "            if re.search(pattern, filename_lower):\n",
        "                return doc_type\n",
        "\n",
        "        # Classification par contenu (analyse des premi√®res pages)\n",
        "        content_keywords = self._extract_content_keywords()\n",
        "        if 'valeur' in content_keywords and 'limite' in content_keywords:\n",
        "            if 'atmosph√©rique' in content_keywords or 'air' in content_keywords:\n",
        "                return 'vlg_atmospherique'\n",
        "            elif 'liquide' in content_keywords or 'eau' in content_keywords:\n",
        "                return 'vlg_liquide'\n",
        "\n",
        "        return 'autre'\n",
        "\n",
        "    def _extract_content_keywords(self) -> List[str]:\n",
        "        \"\"\"Extrait les mots-cl√©s des premi√®res pages pour la classification\"\"\"\n",
        "        keywords = []\n",
        "        for page_num in range(min(3, len(self.doc))):\n",
        "            page = self.doc.load_page(page_num)\n",
        "            text = page.get_text().lower()\n",
        "\n",
        "            # Extraction de mots-cl√©s pertinents\n",
        "            key_terms = [\n",
        "                'valeur', 'limite', 'norme', 'seuil', 'd√©cret',\n",
        "                'atmosph√©rique', 'air', 'liquide', 'eau',\n",
        "                'ciment', 'c√©ramique', 'textile', 'sucre',\n",
        "                'pollution', '√©mission', 'rejet', 'qualit√©'\n",
        "            ]\n",
        "\n",
        "            for term in key_terms:\n",
        "                if term in text:\n",
        "                    keywords.append(term)\n",
        "\n",
        "        return keywords\n",
        "\n",
        "    def _extract_metadata(self) -> Dict[str, Any]:\n",
        "        \"\"\"Extraction enrichie des m√©tadonn√©es\"\"\"\n",
        "        meta = self.doc.metadata\n",
        "        return {\n",
        "            \"title\": meta.get(\"title\", \"\"),\n",
        "            \"author\": meta.get(\"author\", \"\"),\n",
        "            \"creation_date\": self._parse_pdf_date(meta.get(\"creationDate\")),\n",
        "            \"modification_date\": self._parse_pdf_date(meta.get(\"modDate\")),\n",
        "            \"keywords\": meta.get(\"keywords\", \"\"),\n",
        "            \"pages\": len(self.doc),\n",
        "            \"file_size\": os.path.getsize(self.pdf_path),\n",
        "            \"analysis\": self.doc_analysis\n",
        "        }\n",
        "\n",
        "    def _parse_pdf_date(self, date_str: Optional[str]) -> str:\n",
        "        \"\"\"Convertit les dates PDF en format ISO\"\"\"\n",
        "        if not date_str:\n",
        "            return \"\"\n",
        "        try:\n",
        "            if date_str.startswith(\"D:\") and len(date_str) >= 10:\n",
        "                return f\"{date_str[2:6]}-{date_str[6:8]}-{date_str[8:10]}\"\n",
        "        except:\n",
        "            pass\n",
        "        return date_str\n",
        "\n",
        "    def _is_scanned_page(self, page) -> bool:\n",
        "        \"\"\"D√©tection am√©lior√©e des pages scann√©es\"\"\"\n",
        "        # 1. Densit√© de texte tr√®s faible\n",
        "        text = page.get_text()\n",
        "        if len(text.strip()) > 500:  # Seuil plus √©lev√©\n",
        "            return False\n",
        "\n",
        "        # 2. Pr√©sence d'images de grande taille\n",
        "        images = page.get_images()\n",
        "        if not images:\n",
        "            return False\n",
        "\n",
        "        for img in images:\n",
        "            # V√©rification des dimensions et de la r√©solution\n",
        "            if img[2] > 400 and img[3] > 400:  # Largeur et hauteur minimales\n",
        "                return True\n",
        "\n",
        "        # 3. Rapport texte/surface tr√®s faible\n",
        "        text_density = len(text.strip()) / (page.rect.width * page.rect.height) * 10000\n",
        "        return text_density < 2\n",
        "\n",
        "    def _has_complex_tables(self, page) -> bool:\n",
        "        \"\"\"D√©tecte si une page contient des tableaux complexes\"\"\"\n",
        "        try:\n",
        "            tables = page.find_tables()\n",
        "            if not tables:\n",
        "                return False\n",
        "\n",
        "            # V√©rification de la complexit√© des tableaux\n",
        "            for table in tables:\n",
        "                df = table.to_pandas()\n",
        "                if len(df.columns) > 3 and len(df) > 3:\n",
        "                    return True\n",
        "            return False\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _ocr_page(self, page) -> str:\n",
        "        \"\"\"OCR robuste avec gestion d'erreurs\"\"\"\n",
        "        try:\n",
        "            # Extraction avec haute r√©solution\n",
        "            pix = page.get_pixmap(dpi=300)\n",
        "            img_data = pix.tobytes()\n",
        "\n",
        "            if not img_data:\n",
        "                return \"\"\n",
        "\n",
        "            img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "            # Configuration OCR optimis√©e\n",
        "            custom_config = r'--oem 3 --psm 6'\n",
        "\n",
        "            # Essai avec diff√©rentes langues\n",
        "            languages = ['fra', 'ara+fra', 'eng']\n",
        "\n",
        "            for lang in languages:\n",
        "                try:\n",
        "                    text = pytesseract.image_to_string(\n",
        "                        img,\n",
        "                        lang=lang,\n",
        "                        config=custom_config\n",
        "                    )\n",
        "                    if text.strip() and len(text.strip()) > 20:\n",
        "                        return self._clean_text(text)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Dernier recours sans sp√©cification de langue\n",
        "            text = pytesseract.image_to_string(img, config=custom_config)\n",
        "            return self._clean_text(text)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"√âchec OCR page: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Nettoyage approfondi du texte\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remplacement des caract√®res probl√©matiques\n",
        "        replacements = {\n",
        "            '\\xad': '',      # Soft hyphen\n",
        "            '\\uf0b7': '‚Ä¢',   # Bullet\n",
        "            'Ô¨Å': 'fi',\n",
        "            'Ô¨Ç': 'fl',\n",
        "            '\\u200b': '',    # Zero-width space\n",
        "            '\\u202f': ' ',   # Narrow no-break space\n",
        "            '\\ufeff': '',    # BOM\n",
        "        }\n",
        "\n",
        "        for char, replacement in replacements.items():\n",
        "            text = text.replace(char, replacement)\n",
        "\n",
        "        # Normalisation des espaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Suppression des lignes tr√®s courtes (souvent du bruit OCR)\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = [line for line in lines if len(line.strip()) > 2]\n",
        "\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "\n",
        "    def _extract_tables_robust(self, page) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extraction robuste des tableaux avec validation stricte\"\"\"\n",
        "        tables = []\n",
        "\n",
        "        try:\n",
        "            # Tentative d'extraction des tableaux\n",
        "            found_tables = page.find_tables(\n",
        "                strategy=\"lines_strict\",  # Plus strict pour √©viter les hallucinations\n",
        "                snap_tolerance=3.0\n",
        "            )\n",
        "\n",
        "            if not found_tables:\n",
        "                # Tentative avec une strat√©gie alternative\n",
        "                found_tables = page.find_tables(strategy=\"explicit\")\n",
        "\n",
        "            for i, table in enumerate(found_tables):\n",
        "                try:\n",
        "                    df = table.to_pandas()\n",
        "\n",
        "                    # Validation stricte du tableau\n",
        "                    if not self._validate_table_strict(df, page):\n",
        "                        logger.debug(f\"Tableau {i} rejet√© - validation √©chou√©\")\n",
        "                        continue\n",
        "\n",
        "                    # Nettoyage et traitement des cellules fusionn√©es\n",
        "                    df_cleaned = self._process_merged_cells(df)\n",
        "\n",
        "                    # Structure finale du tableau\n",
        "                    table_data = {\n",
        "                        \"table_id\": i,\n",
        "                        \"bbox\": table.bbox,\n",
        "                        \"header\": df_cleaned.columns.tolist(),\n",
        "                        \"rows\": df_cleaned.fillna(\"\").values.tolist(),\n",
        "                        \"shape\": df_cleaned.shape,\n",
        "                        \"confidence\": self._calculate_table_confidence(df_cleaned)\n",
        "                    }\n",
        "\n",
        "                    tables.append(table_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Erreur traitement tableau {i}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erreur extraction tableaux: {str(e)}\")\n",
        "\n",
        "        return tables\n",
        "\n",
        "    def _validate_table_strict(self, df: pd.DataFrame, page) -> bool:\n",
        "        \"\"\"Validation stricte pour √©viter les hallucinations de tableaux\"\"\"\n",
        "        # 1. V√©rification des dimensions\n",
        "        if len(df.columns) > self.config.max_columns:\n",
        "            logger.debug(f\"Trop de colonnes: {len(df.columns)}\")\n",
        "            return False\n",
        "\n",
        "        if len(df) < self.config.min_rows:\n",
        "            logger.debug(f\"Pas assez de lignes: {len(df)}\")\n",
        "            return False\n",
        "\n",
        "        # 2. V√©rification du taux de cellules vides\n",
        "        null_ratio = df.isnull().sum().sum() / df.size\n",
        "        if null_ratio > self.config.max_null_percentage:\n",
        "            logger.debug(f\"Trop de cellules vides: {null_ratio:.2%}\")\n",
        "            return False\n",
        "\n",
        "        # 3. V√©rification du contenu significatif\n",
        "        text_cells = 0\n",
        "        total_cells = df.size\n",
        "\n",
        "        for col in df.columns:\n",
        "            for value in df[col]:\n",
        "                if pd.notna(value) and str(value).strip():\n",
        "                    text_cells += 1\n",
        "\n",
        "        content_ratio = text_cells / total_cells\n",
        "        if content_ratio < self.config.min_content_ratio:\n",
        "            logger.debug(f\"Ratio de contenu trop faible: {content_ratio:.2%}\")\n",
        "            return False\n",
        "\n",
        "        # 4. V√©rification de la coh√©rence des colonnes\n",
        "        if self._has_incoherent_columns(df):\n",
        "            logger.debug(\"Colonnes incoh√©rentes d√©tect√©es\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _has_incoherent_columns(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"D√©tecte les colonnes incoh√©rentes (signe d'hallucination)\"\"\"\n",
        "        for col in df.columns:\n",
        "            # V√©rification si une colonne contient principalement des fragments\n",
        "            values = df[col].dropna().astype(str)\n",
        "            if len(values) > 0:\n",
        "                # Si plus de 80% des valeurs font moins de 3 caract√®res, c'est suspect\n",
        "                short_values = sum(1 for v in values if len(v.strip()) < 3)\n",
        "                if short_values / len(values) > 0.8:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _process_merged_cells(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Traitement intelligent des cellules fusionn√©es\"\"\"\n",
        "        df_copy = df.copy()\n",
        "\n",
        "        # Forward fill pour les colonnes d'en-t√™te (premi√®re colonne g√©n√©ralement)\n",
        "        if len(df_copy.columns) > 0:\n",
        "            df_copy.iloc[:, 0] = df_copy.iloc[:, 0].ffill()\n",
        "\n",
        "        # Traitement sp√©cifique pour les tableaux de normes\n",
        "        if self._is_standards_table(df_copy):\n",
        "            df_copy = self._process_standards_table(df_copy)\n",
        "\n",
        "        return df_copy\n",
        "\n",
        "    def _is_standards_table(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"D√©tecte si c'est un tableau de normes/valeurs limites\"\"\"\n",
        "        # Recherche de mots-cl√©s typiques\n",
        "        keywords = ['polluant', 'limite', 'valeur', 'unit√©', '¬µg/m¬≥', 'mg/l']\n",
        "        text_content = ' '.join([str(col) for col in df.columns]).lower()\n",
        "\n",
        "        return any(keyword in text_content for keyword in keywords)\n",
        "\n",
        "    def _process_standards_table(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Traitement sp√©cialis√© pour les tableaux de normes\"\"\"\n",
        "        # Logic sp√©cifique pour les tableaux de valeurs limites\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Propagation des valeurs dans les cellules fusionn√©es\n",
        "        for col_idx in range(len(df_processed.columns)):\n",
        "            df_processed.iloc[:, col_idx] = df_processed.iloc[:, col_idx].ffill()\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def _calculate_table_confidence(self, df: pd.DataFrame) -> float:\n",
        "        \"\"\"Calcule un score de confiance pour le tableau\"\"\"\n",
        "        score = 1.0\n",
        "\n",
        "        # P√©nalit√© pour les cellules vides\n",
        "        null_ratio = df.isnull().sum().sum() / df.size\n",
        "        score -= null_ratio * 0.5\n",
        "\n",
        "        # Bonus pour la coh√©rence des types de donn√©es\n",
        "        for col in df.columns:\n",
        "            values = df[col].dropna()\n",
        "            if len(values) > 0:\n",
        "                # V√©rification de la coh√©rence des types\n",
        "                numeric_count = sum(1 for v in values if str(v).replace('.', '').replace(',', '').isdigit())\n",
        "                if numeric_count / len(values) > 0.7:  # Colonne majoritairement num√©rique\n",
        "                    score += 0.1\n",
        "\n",
        "        return max(0.0, min(1.0, score))\n",
        "\n",
        "    def _extract_structured_text(self, page) -> Dict[str, Any]:\n",
        "        \"\"\"Extraction de texte structur√© sans bruit excessif\"\"\"\n",
        "        # Utilisation d'une approche par blocs plut√¥t que par span\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "\n",
        "        structured_content = {\n",
        "            \"title_text\": \"\",\n",
        "            \"body_text\": \"\",\n",
        "            \"headers\": [],\n",
        "            \"paragraphs\": []\n",
        "        }\n",
        "\n",
        "        for block in blocks:\n",
        "            if len(block) >= 5:  # Structure valide\n",
        "                text = block[4].strip()\n",
        "                if not text:\n",
        "                    continue\n",
        "\n",
        "                # Classification basique du contenu\n",
        "                if self._is_title_text(text, block):\n",
        "                    structured_content[\"headers\"].append(text)\n",
        "                    if not structured_content[\"title_text\"]:\n",
        "                        structured_content[\"title_text\"] = text\n",
        "                elif len(text) > 50:  # Paragraphe substantiel\n",
        "                    structured_content[\"paragraphs\"].append(text)\n",
        "                    structured_content[\"body_text\"] += text + \"\\n\"\n",
        "\n",
        "        return structured_content\n",
        "\n",
        "    def _is_title_text(self, text: str, block: tuple) -> bool:\n",
        "        \"\"\"D√©tecte si un texte est un titre\"\"\"\n",
        "        # Heuristiques simples pour d√©tecter les titres\n",
        "        if len(text) > 100:  # Trop long pour √™tre un titre\n",
        "            return False\n",
        "\n",
        "        if text.isupper() or text.istitle():\n",
        "            return True\n",
        "\n",
        "        # D√©tection bas√©e sur des mots-cl√©s\n",
        "        title_keywords = ['article', 'chapitre', 'section', 'annexe', 'tableau']\n",
        "        return any(keyword in text.lower() for keyword in title_keywords)\n",
        "\n",
        "    def _detect_pollutants_enhanced(self, text: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"D√©tection am√©lior√©e des polluants avec contexte\"\"\"\n",
        "        found_pollutants = {}\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for code, names in self.POLLUTANT_MAPPING.items():\n",
        "            for name in names:\n",
        "                if name in text_lower:\n",
        "                    # Extraction du contexte autour du polluant\n",
        "                    context = self._extract_context(text_lower, name)\n",
        "\n",
        "                    found_pollutants[code] = {\n",
        "                        \"name\": name,\n",
        "                        \"matched_term\": name,\n",
        "                        \"context\": context,\n",
        "                        \"has_limit_value\": self._has_associated_limit(context)\n",
        "                    }\n",
        "                    break  # Un seul match par polluant\n",
        "\n",
        "        return found_pollutants\n",
        "\n",
        "    def _extract_context(self, text: str, term: str, window: int = 100) -> str:\n",
        "        \"\"\"Extrait le contexte autour d'un terme\"\"\"\n",
        "        pos = text.find(term)\n",
        "        if pos == -1:\n",
        "            return \"\"\n",
        "\n",
        "        start = max(0, pos - window)\n",
        "        end = min(len(text), pos + len(term) + window)\n",
        "\n",
        "        return text[start:end].strip()\n",
        "\n",
        "    def _has_associated_limit(self, context: str) -> bool:\n",
        "        \"\"\"V√©rifie si le contexte contient une valeur limite\"\"\"\n",
        "        # Recherche de patterns num√©riques avec unit√©s\n",
        "        limit_pattern = r'\\d+[\\d\\s,.]*\\s*(¬µg/m¬≥|mg/m¬≥|mg/l|¬µg/l|ppm|ppb)'\n",
        "        return bool(re.search(limit_pattern, context))\n",
        "\n",
        "    def _extract_limit_values_enhanced(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extraction am√©lior√©e des valeurs limites\"\"\"\n",
        "        # Pattern plus sophistiqu√© pour les valeurs limites\n",
        "        patterns = [\n",
        "            r'(\\d+(?:[,.\\s]\\d+)*)\\s*(¬µg/m¬≥|mg/m¬≥|mg/l|¬µg/l|ng/m¬≥|ppm|ppb|¬∞C|%)',\n",
        "            r'(\\d+(?:[,.\\s]\\d+)*)\\s*(microgrammes?|milligrammes?|nanogrammes?)',\n",
        "            r'(\\d+(?:[,.\\s]\\d+)*)\\s*(?:¬µg|mg|ng|ppm|ppb)'\n",
        "        ]\n",
        "\n",
        "        found_values = []\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                value_str = match[0].replace(' ', '').replace(',', '.')\n",
        "                unit = match[1] if len(match) > 1 else 'unit√©_non_sp√©cifi√©e'\n",
        "\n",
        "                try:\n",
        "                    numeric_value = float(value_str)\n",
        "                    found_values.append({\n",
        "                        \"raw_value\": match[0],\n",
        "                        \"numeric_value\": numeric_value,\n",
        "                        \"unit\": unit,\n",
        "                        \"context\": self._extract_context(text, match[0])\n",
        "                    })\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        return found_values\n",
        "\n",
        "    def _process_page_with_strategy(self, page_num: int) -> Dict[str, Any]:\n",
        "        \"\"\"Traite une page avec la strat√©gie appropri√©e\"\"\"\n",
        "        page = self.doc.load_page(page_num)\n",
        "\n",
        "        page_data = {\n",
        "            \"page_number\": page_num + 1,\n",
        "            \"dimensions\": {\"width\": page.rect.width, \"height\": page.rect.height},\n",
        "            \"strategy_used\": \"\",\n",
        "            \"content\": {},\n",
        "            \"confidence\": 0.0\n",
        "        }\n",
        "\n",
        "        # Strat√©gie 1: Page scann√©e -> OCR\n",
        "        if self._is_scanned_page(page):\n",
        "            page_data[\"strategy_used\"] = \"ocr\"\n",
        "            ocr_text = self._ocr_page(page)\n",
        "\n",
        "            if ocr_text:\n",
        "                page_data[\"content\"] = {\n",
        "                    \"text\": ocr_text,\n",
        "                    \"pollutants\": self._detect_pollutants_enhanced(ocr_text),\n",
        "                    \"limit_values\": self._extract_limit_values_enhanced(ocr_text)\n",
        "                }\n",
        "                page_data[\"confidence\"] = 0.6  # OCR moins fiable\n",
        "            else:\n",
        "                page_data[\"content\"] = {\"error\": \"√âchec OCR\"}\n",
        "                page_data[\"confidence\"] = 0.0\n",
        "\n",
        "        # Strat√©gie 2: Page avec tableaux complexes\n",
        "        elif self._has_complex_tables(page):\n",
        "            page_data[\"strategy_used\"] = \"table_extraction\"\n",
        "\n",
        "            tables = self._extract_tables_robust(page)\n",
        "            text_content = self._extract_structured_text(page)\n",
        "\n",
        "            full_text = text_content[\"body_text\"]\n",
        "\n",
        "            page_data[\"content\"] = {\n",
        "                \"tables\": tables,\n",
        "                \"text_structure\": text_content,\n",
        "                \"pollutants\": self._detect_pollutants_enhanced(full_text),\n",
        "                \"limit_values\": self._extract_limit_values_enhanced(full_text)\n",
        "            }\n",
        "\n",
        "            # Calcul de confiance bas√© sur les tableaux\n",
        "            if tables:\n",
        "                avg_confidence = np.mean([t[\"confidence\"] for t in tables])\n",
        "                page_data[\"confidence\"] = avg_confidence\n",
        "            else:\n",
        "                page_data[\"confidence\"] = 0.3\n",
        "\n",
        "        # Strat√©gie 3: Page textuelle standard\n",
        "        else:\n",
        "            page_data[\"strategy_used\"] = \"text_extraction\"\n",
        "            text_content = self._extract_structured_text(page)\n",
        "            full_text = text_content[\"body_text\"]\n",
        "\n",
        "            if full_text.strip():\n",
        "                page_data[\"content\"] = {\n",
        "                    \"text_structure\": text_content,\n",
        "                    \"pollutants\": self._detect_pollutants_enhanced(full_text),\n",
        "                    \"limit_values\": self._extract_limit_values_enhanced(full_text)\n",
        "                }\n",
        "                page_data[\"confidence\"] = 0.9\n",
        "            else:\n",
        "                page_data[\"content\"] = {\"error\": \"Aucun contenu textuel significatif\"}\n",
        "                page_data[\"confidence\"] = 0.1\n",
        "\n",
        "        return page_data\n",
        "\n",
        "    def parse(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyse compl√®te du document avec strat√©gies adaptatives\"\"\"\n",
        "        logger.info(f\"D√©but analyse: {self.filename}\")\n",
        "\n",
        "        doc_data = {\n",
        "            \"metadata\": self.metadata,\n",
        "            \"document_type\": self.doc_type,\n",
        "            \"filename\": self.filename,\n",
        "            \"analysis_summary\": {\n",
        "                \"total_pages\": len(self.doc),\n",
        "                \"strategies_used\": {},\n",
        "                \"avg_confidence\": 0.0,\n",
        "                \"processing_errors\": 0\n",
        "            },\n",
        "            \"pages\": []\n",
        "        }\n",
        "\n",
        "        # Traitement des pages\n",
        "        confidences = []\n",
        "        strategies = {}\n",
        "        errors = 0\n",
        "\n",
        "        for page_num in range(len(self.doc)):\n",
        "            try:\n",
        "                page_data = self._process_page_with_strategy(page_num)\n",
        "                doc_data[\"pages\"].append(page_data)\n",
        "\n",
        "                # Collecte des statistiques\n",
        "                confidences.append(page_data[\"confidence\"])\n",
        "                strategy = page_data[\"strategy_used\"]\n",
        "                strategies[strategy] = strategies.get(strategy, 0) + 1\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erreur page {page_num + 1}: {str(e)}\")\n",
        "                doc_data[\"pages\"].append({\n",
        "                    \"page_number\": page_num + 1,\n",
        "                    \"error\": str(e),\n",
        "                    \"strategy_used\": \"error\",\n",
        "                    \"confidence\": 0.0\n",
        "                })\n",
        "                errors += 1\n",
        "\n",
        "        # R√©sum√© de l'analyse\n",
        "        doc_data[\"analysis_summary\"].update({\n",
        "            \"strategies_used\": strategies,\n",
        "            \"avg_confidence\": np.mean(confidences) if confidences else 0.0,\n",
        "            \"processing_errors\": errors\n",
        "        })\n",
        "\n",
        "        # Analyse globale des polluants\n",
        "        doc_data[\"global_analysis\"] = self._analyze_document_globally(doc_data)\n",
        "\n",
        "        logger.info(f\"Analyse termin√©e - Confiance: {doc_data['analysis_summary']['avg_confidence']:.2f}\")\n",
        "\n",
        "        return doc_data\n",
        "\n",
        "    def _analyze_document_globally(self, doc_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Analyse globale du document\"\"\"\n",
        "        global_pollutants = {}\n",
        "        global_limits = []\n",
        "\n",
        "        for page in doc_data[\"pages\"]:\n",
        "            if \"content\" in page and isinstance(page[\"content\"], dict):\n",
        "                # Agr√©gation des polluants\n",
        "                if \"pollutants\" in page[\"content\"]:\n",
        "                    for code, info in page[\"content\"][\"pollutants\"].items():\n",
        "                        if code not in global_pollutants:\n",
        "                            global_pollutants[code] = {\n",
        "                                \"name\": info[\"name\"],\n",
        "                                \"pages\": [page[\"page_number\"]],\n",
        "                                \"contexts\": [info.get(\"context\", \"\")],\n",
        "                                \"has_limits\": info.get(\"has_limit_value\", False)\n",
        "                            }\n",
        "                        else:\n",
        "                            global_pollutants[code][\"pages\"].append(page[\"page_number\"])\n",
        "                            global_pollutants[code][\"contexts\"].append(info.get(\"context\", \"\"))\n",
        "\n",
        "                # Agr√©gation des valeurs limites\n",
        "                if \"limit_values\" in page[\"content\"]:\n",
        "                    for limit in page[\"content\"][\"limit_values\"]:\n",
        "                        limit[\"page\"] = page[\"page_number\"]\n",
        "                        global_limits.append(limit)\n",
        "\n",
        "        return {\n",
        "            \"pollutants_summary\": global_pollutants,\n",
        "            \"limit_values_summary\": global_limits,\n",
        "            \"document_quality\": self._assess_document_quality(doc_data),\n",
        "            \"extraction_recommendations\": self._generate_recommendations(doc_data)\n",
        "        }\n",
        "\n",
        "    def _assess_document_quality(self, doc_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"√âvalue la qualit√© de l'extraction\"\"\"\n",
        "        total_pages = len(doc_data[\"pages\"])\n",
        "        successful_pages = sum(1 for p in doc_data[\"pages\"] if p.get(\"confidence\", 0) > 0.5)\n",
        "\n",
        "        quality_score = successful_pages / total_pages if total_pages > 0 else 0\n",
        "\n",
        "        return {\n",
        "            \"overall_score\": quality_score,\n",
        "            \"successful_pages\": successful_pages,\n",
        "            \"total_pages\": total_pages,\n",
        "            \"quality_level\": \"high\" if quality_score > 0.8 else \"medium\" if quality_score > 0.5 else \"low\"\n",
        "        }\n",
        "\n",
        "    def _generate_recommendations(self, doc_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"G√©n√®re des recommandations pour am√©liorer l'extraction\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Analyse des strat√©gies utilis√©es\n",
        "        strategies = doc_data[\"analysis_summary\"][\"strategies_used\"]\n",
        "        errors = doc_data[\"analysis_summary\"][\"processing_errors\"]\n",
        "\n",
        "        if strategies.get(\"ocr\", 0) > strategies.get(\"text_extraction\", 0):\n",
        "            recommendations.append(\"Document principalement scann√© - consid√©rer une version native si disponible\")\n",
        "\n",
        "        if errors > 0:\n",
        "            recommendations.append(f\"{errors} pages ont √©chou√© - v√©rifier la qualit√© du PDF\")\n",
        "\n",
        "        if doc_data[\"analysis_summary\"][\"avg_confidence\"] < 0.6:\n",
        "            recommendations.append(\"Confiance faible - r√©vision manuelle recommand√©e\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "\n",
        "def process_pdf_batch(pdf_files: List[str], config: Optional[TableValidationConfig] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Traite un batch de fichiers PDF avec rapports d√©taill√©s\"\"\"\n",
        "    results = {}\n",
        "    processing_summary = {\n",
        "        \"total_files\": len(pdf_files),\n",
        "        \"successful\": 0,\n",
        "        \"failed\": 0,\n",
        "        \"avg_confidence\": 0.0,\n",
        "        \"processing_time\": 0.0\n",
        "    }\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            logger.info(f\"Traitement: {pdf_path}\")\n",
        "\n",
        "            # V√©rification de l'existence du fichier\n",
        "            if not os.path.exists(pdf_path):\n",
        "                logger.error(f\"Fichier introuvable: {pdf_path}\")\n",
        "                results[pdf_path] = {\"error\": \"Fichier introuvable\"}\n",
        "                processing_summary[\"failed\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Traitement du PDF\n",
        "            parser = RobustPDFParser(pdf_path, config)\n",
        "            result = parser.parse()\n",
        "            results[pdf_path] = result\n",
        "\n",
        "            # Mise √† jour des statistiques\n",
        "            processing_summary[\"successful\"] += 1\n",
        "            processing_summary[\"avg_confidence\"] += result[\"analysis_summary\"][\"avg_confidence\"]\n",
        "\n",
        "            # Affichage des r√©sultats\n",
        "            doc_type = result[\"document_type\"]\n",
        "            strategies = result[\"analysis_summary\"][\"strategies_used\"]\n",
        "            confidence = result[\"analysis_summary\"][\"avg_confidence\"]\n",
        "            pollutants_count = len(result[\"global_analysis\"][\"pollutants_summary\"])\n",
        "\n",
        "            logger.info(f\"‚úì Succ√®s: {result['metadata']['pages']} pages\")\n",
        "            logger.info(f\"  Type: {doc_type}\")\n",
        "            logger.info(f\"  Strat√©gies: {strategies}\")\n",
        "            logger.info(f\"  Confiance: {confidence:.2f}\")\n",
        "            logger.info(f\"  Polluants: {pollutants_count}\")\n",
        "\n",
        "            # Affichage des recommandations\n",
        "            recommendations = result[\"global_analysis\"][\"extraction_recommendations\"]\n",
        "            if recommendations:\n",
        "                logger.warning(\"  Recommandations:\")\n",
        "                for rec in recommendations:\n",
        "                    logger.warning(f\"    - {rec}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erreur critique: {pdf_path} - {str(e)}\")\n",
        "            results[pdf_path] = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
        "            processing_summary[\"failed\"] += 1\n",
        "\n",
        "    # Finalisation des statistiques\n",
        "    processing_summary[\"processing_time\"] = time.time() - start_time\n",
        "    if processing_summary[\"successful\"] > 0:\n",
        "        processing_summary[\"avg_confidence\"] /= processing_summary[\"successful\"]\n",
        "\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"summary\": processing_summary\n",
        "    }\n",
        "\n",
        "\n",
        "def validate_parsing_setup():\n",
        "    \"\"\"Valide la configuration de l'environnement de parsing\"\"\"\n",
        "    validation_results = {\n",
        "        \"tesseract_available\": False,\n",
        "        \"tesseract_languages\": [],\n",
        "        \"pymupdf_version\": None,\n",
        "        \"pandas_version\": None,\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "\n",
        "    # V√©rification de Tesseract\n",
        "    try:\n",
        "        version = pytesseract.get_tesseract_version()\n",
        "        validation_results[\"tesseract_available\"] = True\n",
        "\n",
        "        # Test des langues disponibles\n",
        "        try:\n",
        "            langs = pytesseract.get_languages(config='')\n",
        "            validation_results[\"tesseract_languages\"] = langs\n",
        "        except:\n",
        "            validation_results[\"tesseract_languages\"] = [\"eng\"]  # Par d√©faut\n",
        "\n",
        "    except Exception as e:\n",
        "        validation_results[\"recommendations\"].append(\n",
        "            \"Tesseract non trouv√© - installer avec: apt-get install tesseract-ocr tesseract-ocr-fra\"\n",
        "        )\n",
        "\n",
        "    # V√©rification des versions des d√©pendances\n",
        "    try:\n",
        "        validation_results[\"pymupdf_version\"] = fitz.__version__\n",
        "        validation_results[\"pandas_version\"] = pd.__version__\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Recommandations additionnelles\n",
        "    if \"fra\" not in validation_results[\"tesseract_languages\"]:\n",
        "        validation_results[\"recommendations\"].append(\n",
        "            \"Pack fran√ßais non trouv√© - installer avec: apt-get install tesseract-ocr-fra\"\n",
        "        )\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "\n",
        "# Configuration d'exemple pour diff√©rents types de documents\n",
        "class DocumentConfigs:\n",
        "    \"\"\"Configurations pr√©d√©finies pour diff√©rents types de documents\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_config(doc_type: str) -> TableValidationConfig:\n",
        "        \"\"\"Retourne une configuration optimis√©e selon le type de document\"\"\"\n",
        "        configs = {\n",
        "            \"vlg_atmospherique\": TableValidationConfig(\n",
        "                max_columns=8,\n",
        "                min_rows=3,\n",
        "                max_null_percentage=0.3,\n",
        "                min_content_ratio=0.4\n",
        "            ),\n",
        "            \"vlg_liquide\": TableValidationConfig(\n",
        "                max_columns=10,\n",
        "                min_rows=5,\n",
        "                max_null_percentage=0.2,\n",
        "                min_content_ratio=0.5\n",
        "            ),\n",
        "            \"normes_air\": TableValidationConfig(\n",
        "                max_columns=6,\n",
        "                min_rows=4,\n",
        "                max_null_percentage=0.1,\n",
        "                min_content_ratio=0.6\n",
        "            ),\n",
        "            \"vls_ciment\": TableValidationConfig(\n",
        "                max_columns=12,\n",
        "                min_rows=3,\n",
        "                max_null_percentage=0.4,\n",
        "                min_content_ratio=0.3\n",
        "            ),\n",
        "            \"default\": TableValidationConfig()\n",
        "        }\n",
        "\n",
        "        return configs.get(doc_type, configs[\"default\"])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Validation de l'environnement\n",
        "    print(\"=\"*60)\n",
        "    print(\"VALIDATION DE L'ENVIRONNEMENT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    validation = validate_parsing_setup()\n",
        "    print(f\"Tesseract disponible: {validation['tesseract_available']}\")\n",
        "    print(f\"Langues OCR: {validation['tesseract_languages']}\")\n",
        "    print(f\"PyMuPDF version: {validation['pymupdf_version']}\")\n",
        "\n",
        "    if validation[\"recommendations\"]:\n",
        "        print(\"\\nRecommandations:\")\n",
        "        for rec in validation[\"recommendations\"]:\n",
        "            print(f\"  - {rec}\")\n",
        "\n",
        "    # Configuration Tesseract (adapter selon votre installation)\n",
        "    pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "    # Liste des fichiers √† traiter\n",
        "    pdf_files = pdf_names\n",
        "    pdf_files = [f\"/content{file}\" for file in pdf_files]\n",
        "\n",
        "    # Traitement avec configuration personnalis√©e\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"D√âBUT DU TRAITEMENT DES DOCUMENTS PDF\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Configuration globale (peut √™tre ajust√©e par document)\n",
        "    default_config = TableValidationConfig(\n",
        "        max_columns=10,\n",
        "        min_rows=2,\n",
        "        max_null_percentage=0.4,\n",
        "        min_content_ratio=0.3\n",
        "    )\n",
        "\n",
        "    # Traitement du batch\n",
        "    batch_results = process_pdf_batch(pdf_files, default_config)\n",
        "\n",
        "    # Sauvegarde des r√©sultats\n",
        "    output_file = \"robust_parsing_results.json\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(batch_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "    # Rapport final\n",
        "    summary = batch_results[\"summary\"]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RAPPORT FINAL\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Fichiers trait√©s: {summary['total_files']}\")\n",
        "    print(f\"Succ√®s: {summary['successful']}\")\n",
        "    print(f\"√âchecs: {summary['failed']}\")\n",
        "    print(f\"Confiance moyenne: {summary['avg_confidence']:.2f}\")\n",
        "    print(f\"Temps de traitement: {summary['processing_time']:.1f}s\")\n",
        "    print(f\"R√©sultats sauvegard√©s: {output_file}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # G√©n√©ration d'un rapport de qualit√©\n",
        "    quality_report = {\n",
        "        \"high_quality_docs\": [],\n",
        "        \"medium_quality_docs\": [],\n",
        "        \"low_quality_docs\": []\n",
        "    }\n",
        "\n",
        "    for file_path, result in batch_results[\"results\"].items():\n",
        "        if \"error\" not in result:\n",
        "            quality = result[\"global_analysis\"][\"document_quality\"][\"quality_level\"]\n",
        "            filename = os.path.basename(file_path)\n",
        "\n",
        "            if quality == \"high\":\n",
        "                quality_report[\"high_quality_docs\"].append(filename)\n",
        "            elif quality == \"medium\":\n",
        "                quality_report[\"medium_quality_docs\"].append(filename)\n",
        "            else:\n",
        "                quality_report[\"low_quality_docs\"].append(filename)\n",
        "\n",
        "    print(f\"\\nRAPPORT DE QUALIT√â:\")\n",
        "    print(f\"Documents haute qualit√© ({len(quality_report['high_quality_docs'])}): {quality_report['high_quality_docs']}\")\n",
        "    print(f\"Documents qualit√© moyenne ({len(quality_report['medium_quality_docs'])}): {quality_report['medium_quality_docs']}\")\n",
        "    print(f\"Documents basse qualit√© ({len(quality_report['low_quality_docs'])}): {quality_report['low_quality_docs']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8BuLadXLeHH",
        "outputId": "b9358a08-ee6d-4544-b1fc-e2f7fe178195"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "VALIDATION DE L'ENVIRONNEMENT\n",
            "============================================================\n",
            "Tesseract disponible: True\n",
            "Langues OCR: ['eng', 'osd']\n",
            "PyMuPDF version: 1.26.3\n",
            "\n",
            "Recommandations:\n",
            "  - Pack fran√ßais non trouv√© - installer avec: apt-get install tesseract-ocr-fra\n",
            "\n",
            "============================================================\n",
            "D√âBUT DU TRAITEMENT DES DOCUMENTS PDF\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RAPPORT FINAL\n",
            "============================================================\n",
            "Fichiers trait√©s: 17\n",
            "Succ√®s: 17\n",
            "√âchecs: 0\n",
            "Confiance moyenne: 0.74\n",
            "Temps de traitement: 39.4s\n",
            "R√©sultats sauvegard√©s: robust_parsing_results.json\n",
            "============================================================\n",
            "\n",
            "RAPPORT DE QUALIT√â:\n",
            "Documents haute qualit√© (13): ['Valeurs_limites_sectorielles__c√©ramique.pdf', 'Normes_de_la_qualit√©_de_lair.pdf', 'VLG_2018_des_rejets_industriels__liquides.pdf', 'valeurs_limites_sp√©cifiques_du_secteur_cimentier.pdf', 'VLS_du_secteur_peinture_et_vernis.pdf', 'VLS__du_secteur_c√©ramique.pdf', 'LETTRE_ROYALE.pdf', 'VLS_des_rejets_domestiques.pdf', 'decretCNE.pdf', 'VLS_du_secteur_cimentier.pdf', 'Seuils_dinformation_et_seuils_dalerte.pdf', 'VLS_du_secteur_textile.pdf', 'VLS_du__papier_cartons.pdf']\n",
            "Documents qualit√© moyenne (2): ['Valeurs_limites_g√©n√©rales_des_rejets_atmosph√©riques.pdf', 'Normes_qualit√©__des_eaux_us√©es_√©pur√©es_detinees_a_l_irrigation.pdf']\n",
            "Documents basse qualit√© (2): ['VLS__de_traitement_du_surface_1.pdf', 'VLS_de_lindustrie_du_sucre.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Processing"
      ],
      "metadata": {
        "id": "CWzRVYuIMlPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "PvJCevUMoj6O"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY = \"AIzaSyDQ9k4tK43Un-dxkAKjKHaCzOaMrzfIoxI\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.pagesizes import landscape ,A3 , A4\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image ,Flowable\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch, cm , mm\n",
        "from reportlab.pdfbase.pdfmetrics import stringWidth\n",
        "from reportlab.lib.enums import TA_LEFT, TA_CENTER\n",
        "from reportlab.platypus import PageBreak\n"
      ],
      "metadata": {
        "id": "QS1dQnHkzvT5"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. SETUP: Configure the Gemini API and DataFrame Structure ---\n",
        "\n",
        "# IMPORTANT: Replace \"YOUR_API_KEY\" with your actual Gemini API key\n",
        "try:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API. Please check your API key. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Define the final 3-level hierarchical column structure\n",
        "header = [\n",
        "    ('Commitment Register Overview', 'Register Identifier', ''),\n",
        "    ('Commitment Register Overview', 'Commitment Identifier', ''),\n",
        "    ('Commitment Register Overview', 'Commitment or Obligation', ''),\n",
        "    ('Commitment Register Overview', 'Description', ''),\n",
        "    ('Commitment Register Overview', 'Project Phase', ''),\n",
        "    ('Commitment Management', 'Potential Impact on Scope?', ''),\n",
        "    ('Commitment Management', 'Status', ''),\n",
        "    ('Commitment Management', 'Commitment Deadline', ''),\n",
        "    ('Commitment Management', 'First Lead', ''),\n",
        "    ('Commitment Management', 'Second Lead', ''),\n",
        "    ('Commitment Management', 'Third Lead', ''),\n",
        "    ('Commitment Management', 'Primary Commitment Documentation', ''),\n",
        "    ('Commitment Management', 'Impact or Hazard Addressed', ''),\n",
        "    ('Commitment Management', 'Approving Agencies', ''),\n",
        "    ('Commitment Management', 'Other Stakeholders', ''),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Preparation/construction'),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Operation'),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Input Management'),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Discharge management'),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Off-Sites'),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Other'),\n",
        "    ('Commitment Management', 'Affected Areas or Processes', 'Fungibility'),\n",
        "    ('Commitment Management', 'Impact', 'CAPEX'),\n",
        "    ('Commitment Management', 'Impact', 'OPEX'),\n",
        "    ('Commitment Management', 'Impact', 'Health & Safety'),\n",
        "    ('Commitment Management', 'Impact', 'Social'),\n",
        "    ('Commitment Management', 'Impact', 'Economic'),\n",
        "    ('Commitment Management', 'Impact', 'Environmental'),\n",
        "    ('Commitment Management', 'Impact', 'Regulatory'),\n",
        "    ('Commitment Management', 'Comments', ''),\n",
        "    ('Commitment Management', 'Requires Change Order?', '')\n",
        "]\n",
        "columns = pd.MultiIndex.from_tuples(header)\n",
        "\n",
        "\n",
        "# --- 2. DATA PREPARATION: Functions to load and search the knowledge base ---\n",
        "\n",
        "def load_and_prepare_knowledge_base(json_file_path):\n",
        "    # This function remains unchanged\n",
        "    try:\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {json_file_path} was not found.\")\n",
        "        return None\n",
        "    knowledge_base = []\n",
        "    for doc_path, doc_data in data['results'].items():\n",
        "        full_text = \"\"\n",
        "        for page in doc_data.get('pages', []):\n",
        "            if 'content' in page and 'text_structure' in page['content']:\n",
        "                full_text += page['content']['text_structure'].get('title_text', '') + \"\\n\"\n",
        "                full_text += page['content']['text_structure'].get('body_text', '') + \"\\n\"\n",
        "        if full_text:\n",
        "            knowledge_base.append({\n",
        "                \"filename\": doc_data.get('filename', 'N/A'),\n",
        "                \"document_type\": doc_data.get('document_type', 'N/A'),\n",
        "                \"content\": full_text.lower()\n",
        "            })\n",
        "    print(f\"Knowledge base created with {len(knowledge_base)} documents.\")\n",
        "    return knowledge_base\n",
        "\n",
        "def find_relevant_documents(commitment_description, knowledge_base):\n",
        "    # This function remains unchanged\n",
        "    commitment_description = commitment_description.lower()\n",
        "    keywords = set(re.findall(r'\\b[a-zA-Z√ß√©√†√®√π√¢√™√Æ√¥√ª√¶≈ì\\d]{4,}\\b', commitment_description))\n",
        "    relevant_texts = []\n",
        "    for doc in knowledge_base:\n",
        "        if any(keyword in doc['filename'].lower() for keyword in keywords) or \\\n",
        "           any(keyword in doc['content'] for keyword in keywords):\n",
        "            relevant_texts.append(f\"--- START OF RELEVANT DOCUMENT ({doc['filename']}) ---\\n{doc['content']}\\n--- END OF DOCUMENT ---\\n\")\n",
        "    return \"\\n\".join(relevant_texts)\n",
        "\n",
        "\n",
        "# --- 3. CORE LOGIC: Updated function to call Gemini with all context ---\n",
        "\n",
        "def call_gemini_to_complete_row(commitment_row, relevant_texts, project_description):\n",
        "    \"\"\"Crafts the prompt with all context and calls the Gemini API.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "    commitment_desc = commitment_row[('Commitment Register Overview', 'Description', '')]\n",
        "    commitment_id = commitment_row[('Commitment Register Overview', 'Commitment Identifier', '')]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert Moroccan environmental and project management compliance analyst.\n",
        "    Your task is to complete a row in a project's Commitment Register by synthesizing information from three sources: the project's description, the specific commitment, and relevant legal documents.\n",
        "\n",
        "    **SOURCE 1: PROJECT DESCRIPTION**\n",
        "    {project_description}\n",
        "\n",
        "    **SOURCE 2: COMMITMENT CONTEXT**\n",
        "    - Commitment ID: \"{commitment_id}\"\n",
        "    - Commitment Description: \"{commitment_desc}\"\n",
        "\n",
        "    **SOURCE 3: LEGAL EVIDENCE (Relevant Moroccan regulations and laws)**\n",
        "    {relevant_texts}\n",
        "\n",
        "    **TASK:**\n",
        "    Based on ALL THREE sources provided, analyze how the commitment relates to the project phase, its objectives, and the legal requirements. Then, fill in the following fields. If information is not available, return an empty string \"\". Be concise and accurate. Output ONLY a valid JSON object, with no other text or markdown.\n",
        "\n",
        "    **OUTPUT FORMAT (JSON ONLY):**\n",
        "    {{\n",
        "      \"Impact or Hazard Addressed\": \"Identify the specific risk or hazard. Example: 'Risk of air pollution from emissions exceeding legal limits during the operational phase.'\",\n",
        "      \"Approving Agencies\": \"List the relevant government bodies mentioned. Example: 'Ministry of Energy Transition, Authorities coordinated by the Customer'\",\n",
        "      \"Comments\": \"Provide a brief analysis connecting the commitment to the law and project phase. Mention specific limit values if found. Example: 'As the project is in the FEED phase, this commitment ensures compliance with Law 13-03 is designed in from the start. VLG for SO2 is 500 mg/m3.'\",\n",
        "      \"Affected_Preparation_Construction\": \"Enter 'x' if relevant, otherwise ''\",\n",
        "      \"Affected_Operation\": \"Enter 'x' if relevant, otherwise ''\",\n",
        "      \"Affected_Discharge_Management\": \"Enter 'x' if relevant, otherwise ''\",\n",
        "      \"Impact_Health_Safety\": \"Enter 'x' if relevant, otherwise ''\",\n",
        "      \"Impact_Environmental\": \"Enter 'x' if relevant, otherwise ''\",\n",
        "      \"Impact_Regulatory\": \"Enter 'x' if relevant, otherwise ''\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        cleaned_response = response.text.strip().lstrip(\"```json\").rstrip(\"```\")\n",
        "        return json.loads(cleaned_response)\n",
        "    except json.JSONDecodeEror:\n",
        "        tqdm.write(f\"Error: Gemini returned a non-JSON response for '{commitment_id}':\\n{response.text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"An unexpected error occurred with Gemini API for '{commitment_id}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- 4. MAIN EXECUTION SCRIPT ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the static project description context\n",
        "    project_description_context = \"\"\"\n",
        "    JESA has entered a reimbursable Work Order for elaborating the FEED (Evaluate+ Define) for this project.\n",
        "    This revision of the work order will include the remaining activities and deliverables required to complete Evaluate phase and launch critical packages. It includes also ESIA preparation, architectural activities and deliverables for non-process building and master plan.\n",
        "    Division of Responsibilities: JESA has an EPCM reimbursable scope, however, the Customer still has some responsibilities including coordination with authorities and OCP entities.\n",
        "    Project Objectives: The key objectives of the current phase are Phase 2 (Evaluate) development study and a Class 4 estimate (+/- 20% to +/- 30%). Activities added to the evaluate phase for fast-tracking include: Civil early works, Geo scan, Storage building, Environmental deliverables, and Mechanical ITBs for LLIs.\n",
        "    \"\"\"\n",
        "\n",
        "    knowledge_base = load_and_prepare_knowledge_base('robust_parsing_results.json')\n",
        "    if knowledge_base is None:\n",
        "        exit()\n",
        "\n",
        "    initial_commitments_data = [\n",
        "        [\"Moroccan environmental regulation\", \"Law n¬∞ 13-03 relating to the fight against air pollution\", \"Legal obligation\", \"Controlling the atmospheric emissions during industrial operations while ensuring good air quality.\", \"Design/Operation\", \"High\", \"In Progress\", \"During operational phase\", \"HSE Client\", \"Environment Client\", \"\", \"Environmental Report\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
        "        [\"OCP Group objectives and commitment\", \"Liquid effluents policy\", \"Commitment\", \"Complying with legal regulatory national and international requirements for liquid discharge to ensure the prevention and control of related environmental risks.\", \"Design/Construction/Operation\", \"High\", \"In Progress\", \"During Design, construction and operation phases\", \"Process Engineering\", \"Environmental Engineering\", \"Civil Engineering\", \"Environmental Design criteria; Liquid effluents policy (OCP)\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
        "        [\"OCP Group objectives and commitment\", \"Waste management policy\", \"Commitment\", \"Responsibly manage the waste generated by the project, respecting national and internationally recognized guidelines.\", \"Design/Construction/Operation\", \"High\", \"In Progress\", \"During design Construction and operation phases\", \"Environmental Engineering\", \"HSE Client\", \"Environment-Construction Contractors\", \"Waste management plan\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    ]\n",
        "    df_initial = pd.DataFrame(initial_commitments_data, columns=columns)\n",
        "    df_final = df_initial.copy()\n",
        "\n",
        "    # Wrap the loop with tqdm for a progress bar\n",
        "    for index, row in tqdm(df_initial.iterrows(), total=len(df_initial), desc=\"Analyzing Commitments\"):\n",
        "        commitment_description = row[('Commitment Register Overview', 'Description', '')]\n",
        "\n",
        "        relevant_texts = find_relevant_documents(commitment_description, knowledge_base)\n",
        "\n",
        "        if not relevant_texts:\n",
        "            tqdm.write(f\"  -> No relevant documents for '{row[('Commitment Register Overview', 'Commitment Identifier', '')]}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Call the updated function with the project description\n",
        "        extracted_data = call_gemini_to_complete_row(row, relevant_texts, project_description_context)\n",
        "\n",
        "        time.sleep(2) # Respect API rate limits\n",
        "\n",
        "        if extracted_data:\n",
        "            df_final.loc[index, ('Commitment Management', 'Impact or Hazard Addressed', '')] = extracted_data.get(\"Impact or Hazard Addressed\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Approving Agencies', '')] = extracted_data.get(\"Approving Agencies\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Comments', '')] = extracted_data.get(\"Comments\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Affected Areas or Processes', 'Preparation/construction')] = extracted_data.get(\"Affected_Preparation_Construction\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Affected Areas or Processes', 'Operation')] = extracted_data.get(\"Affected_Operation\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Affected Areas or Processes', 'Discharge management')] = extracted_data.get(\"Affected_Discharge_Management\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Impact', 'Health & Safety')] = extracted_data.get(\"Impact_Health_Safety\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Impact', 'Environmental')] = extracted_data.get(\"Impact_Environmental\", \"\")\n",
        "            df_final.loc[index, ('Commitment Management', 'Impact', 'Regulatory')] = extracted_data.get(\"Impact_Regulatory\", \"\")\n",
        "        else:\n",
        "            tqdm.write(f\"  -> Failed to get a valid response from Gemini for '{row[('Commitment Register Overview', 'Commitment Identifier', '')]}'.\")\n",
        "\n",
        "    print(\"\\n\\n### --- FINAL DYNAMICALLY COMPLETED COMMITMENT REGISTER --- ###\")\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', 2200)\n",
        "    display(df_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "kTmOhtVdooA5",
        "outputId": "0707ee2a-6d65-4cd5-9658-03487b2d04da"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API configured successfully.\n",
            "Knowledge base created with 13 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Commitments:   0%|          | 0/3 [00:00<?, ?steps/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> No relevant documents for 'Law n¬∞ 13-03 relating to the fight against air pollution'. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Commitments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:28<00:00,  9.63s/steps]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "### --- FINAL DYNAMICALLY COMPLETED COMMITMENT REGISTER --- ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          Commitment Register Overview                                                                                                                                                                    Commitment Management                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
              "                   Register Identifier                              Commitment Identifier Commitment or Obligation                                        Description                  Project Phase Potential Impact on Scope?       Status                               Commitment Deadline                 First Lead                Second Lead                            Third Lead                   Primary Commitment Documentation                         Impact or Hazard Addressed                                 Approving Agencies Other Stakeholders Affected Areas or Processes                                                                             Impact                                                                                                         Comments Requires Change Order?\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Preparation/construction Operation Input Management Discharge management Off-Sites Other Fungibility  CAPEX OPEX Health & Safety Social Economic Environmental Regulatory                                                                          \n",
              "0    Moroccan environmental regulation  Law n¬∞ 13-03 relating to the fight against air...         Legal obligation  Controlling the atmospheric emissions during i...               Design/Operation                       High  In Progress                          During operational phase                 HSE Client         Environment Client                                                                     Environmental Report                                                                                                                                                                                                                                                                                                                                                                                \n",
              "1  OCP Group objectives and commitment                            Liquid effluents policy               Commitment  Complying with legal regulatory national and i...  Design/Construction/Operation                       High  In Progress  During Design, construction and operation phases        Process Engineering  Environmental Engineering                     Civil Engineering  Environmental Design criteria; Liquid effluent...  Risk of water pollution from liquid effluents/...  Ministry of Environment (via the CNE), Nationa...                                              x         x                                     x                                                       x                             x          x  As the project is in the FEED/Evaluate phase a...                       \n",
              "2  OCP Group objectives and commitment                            Waste management policy               Commitment  Responsibly manage the waste generated by the ...  Design/Construction/Operation                       High  In Progress   During design Construction and operation phases  Environmental Engineering                 HSE Client  Environment-Construction Contractors                              Waste management plan  Risk of uncontrolled waste generation leading ...  Ministry of Environment (via CNE presidency), ...                                              x         x                                     x                                                       x                             x          x  This commitment aligns with Morocco's foundati...                       "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77d3ca01-0f5b-4136-86f0-b7036ed9bfe2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"5\" halign=\"left\">Commitment Register Overview</th>\n",
              "      <th colspan=\"26\" halign=\"left\">Commitment Management</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Register Identifier</th>\n",
              "      <th>Commitment Identifier</th>\n",
              "      <th>Commitment or Obligation</th>\n",
              "      <th>Description</th>\n",
              "      <th>Project Phase</th>\n",
              "      <th>Potential Impact on Scope?</th>\n",
              "      <th>Status</th>\n",
              "      <th>Commitment Deadline</th>\n",
              "      <th>First Lead</th>\n",
              "      <th>Second Lead</th>\n",
              "      <th>Third Lead</th>\n",
              "      <th>Primary Commitment Documentation</th>\n",
              "      <th>Impact or Hazard Addressed</th>\n",
              "      <th>Approving Agencies</th>\n",
              "      <th>Other Stakeholders</th>\n",
              "      <th colspan=\"7\" halign=\"left\">Affected Areas or Processes</th>\n",
              "      <th colspan=\"7\" halign=\"left\">Impact</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Requires Change Order?</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Preparation/construction</th>\n",
              "      <th>Operation</th>\n",
              "      <th>Input Management</th>\n",
              "      <th>Discharge management</th>\n",
              "      <th>Off-Sites</th>\n",
              "      <th>Other</th>\n",
              "      <th>Fungibility</th>\n",
              "      <th>CAPEX</th>\n",
              "      <th>OPEX</th>\n",
              "      <th>Health &amp; Safety</th>\n",
              "      <th>Social</th>\n",
              "      <th>Economic</th>\n",
              "      <th>Environmental</th>\n",
              "      <th>Regulatory</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Moroccan environmental regulation</td>\n",
              "      <td>Law n¬∞ 13-03 relating to the fight against air...</td>\n",
              "      <td>Legal obligation</td>\n",
              "      <td>Controlling the atmospheric emissions during i...</td>\n",
              "      <td>Design/Operation</td>\n",
              "      <td>High</td>\n",
              "      <td>In Progress</td>\n",
              "      <td>During operational phase</td>\n",
              "      <td>HSE Client</td>\n",
              "      <td>Environment Client</td>\n",
              "      <td></td>\n",
              "      <td>Environmental Report</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OCP Group objectives and commitment</td>\n",
              "      <td>Liquid effluents policy</td>\n",
              "      <td>Commitment</td>\n",
              "      <td>Complying with legal regulatory national and i...</td>\n",
              "      <td>Design/Construction/Operation</td>\n",
              "      <td>High</td>\n",
              "      <td>In Progress</td>\n",
              "      <td>During Design, construction and operation phases</td>\n",
              "      <td>Process Engineering</td>\n",
              "      <td>Environmental Engineering</td>\n",
              "      <td>Civil Engineering</td>\n",
              "      <td>Environmental Design criteria; Liquid effluent...</td>\n",
              "      <td>Risk of water pollution from liquid effluents/...</td>\n",
              "      <td>Ministry of Environment (via the CNE), Nationa...</td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>As the project is in the FEED/Evaluate phase a...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OCP Group objectives and commitment</td>\n",
              "      <td>Waste management policy</td>\n",
              "      <td>Commitment</td>\n",
              "      <td>Responsibly manage the waste generated by the ...</td>\n",
              "      <td>Design/Construction/Operation</td>\n",
              "      <td>High</td>\n",
              "      <td>In Progress</td>\n",
              "      <td>During design Construction and operation phases</td>\n",
              "      <td>Environmental Engineering</td>\n",
              "      <td>HSE Client</td>\n",
              "      <td>Environment-Construction Contractors</td>\n",
              "      <td>Waste management plan</td>\n",
              "      <td>Risk of uncontrolled waste generation leading ...</td>\n",
              "      <td>Ministry of Environment (via CNE presidency), ...</td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>This commitment aligns with Morocco's foundati...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77d3ca01-0f5b-4136-86f0-b7036ed9bfe2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77d3ca01-0f5b-4136-86f0-b7036ed9bfe2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77d3ca01-0f5b-4136-86f0-b7036ed9bfe2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8d85abcd-45dd-46e4-8c8a-b1686f043f12\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d85abcd-45dd-46e4-8c8a-b1686f043f12')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8d85abcd-45dd-46e4-8c8a-b1686f043f12 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_358839d0-f856-4732-b0ca-d202179c7ef1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_final')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_358839d0-f856-4732-b0ca-d202179c7ef1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_final');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Commitment Table\n",
        "import pandas as pd\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Table, TableStyle, Flowable\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.pagesizes import landscape, A3\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.enums import TA_CENTER, TA_LEFT\n",
        "from reportlab.pdfbase.pdfmetrics import stringWidth\n",
        "\n",
        "# --- Your Custom VerticalText Class (Unchanged) ---\n",
        "class VerticalText(Flowable):\n",
        "    \"\"\"A custom flowable to draw text rotated by 90 degrees.\"\"\"\n",
        "    def __init__(self, text, font_name='Helvetica-Bold', font_size=6):\n",
        "        Flowable.__init__(self)\n",
        "        self.text = text\n",
        "        self.font_name = font_name\n",
        "        self.font_size = font_size\n",
        "\n",
        "    def draw(self):\n",
        "        canvas = self.canv\n",
        "        canvas.saveState()\n",
        "        canvas.setFont(self.font_name, self.font_size)\n",
        "        canvas.rotate(90)\n",
        "        canvas.drawString(5, -self.font_size - 2, self.text)\n",
        "        canvas.restoreState()\n",
        "\n",
        "    def wrap(self, available_width, available_height):\n",
        "        text_width = stringWidth(self.text, self.font_name, self.font_size)\n",
        "        return (self.font_size + 4, text_width + 10)\n",
        "\n",
        "def generate_commitment_register_pdf(df, output_filename=\"commitment_register_final.pdf\"):\n",
        "    \"\"\"\n",
        "    Generates a PDF file with a detailed table dynamically from a DataFrame\n",
        "    with a 3-level MultiIndex header.\n",
        "    \"\"\"\n",
        "    pagesize = landscape(A3)\n",
        "    doc = SimpleDocTemplate(\n",
        "        output_filename,\n",
        "        pagesize=pagesize,\n",
        "        rightMargin=0.5 * inch,\n",
        "        leftMargin=0.5 * inch,\n",
        "        topMargin=1.3 * inch,\n",
        "    )\n",
        "\n",
        "    table_width = doc.width\n",
        "\n",
        "    # --- Paragraph Styles for Cell Content (Your Styles) ---\n",
        "    styles = getSampleStyleSheet()\n",
        "    cell_style = ParagraphStyle(\n",
        "        name='CellStyle',\n",
        "        parent=styles['Normal'],\n",
        "        alignment=TA_CENTER,\n",
        "        fontSize=6,\n",
        "        leading=8,\n",
        "        spaceAfter=2,\n",
        "        spaceBefore=2\n",
        "    )\n",
        "    header_style = ParagraphStyle(\n",
        "        name='HeaderStyle',\n",
        "        parent=styles['Normal'],\n",
        "        fontName='Helvetica-Bold',\n",
        "        fontSize=7,\n",
        "        leading=8,\n",
        "        alignment=TA_CENTER,\n",
        "        textColor=colors.white,\n",
        "    )\n",
        "\n",
        "    # --- Helper Functions ---\n",
        "    def create_header_paragraph(text, style=header_style):\n",
        "        return Paragraph(text.replace('\\n', '<br/>'), style)\n",
        "\n",
        "    def create_data_paragraph(text, style=cell_style):\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        return Paragraph(text.replace('\\n', '<br/>').replace('* ', '‚Ä¢ '), style)\n",
        "\n",
        "    # --- DYNAMIC HEADER GENERATION ---\n",
        "    # This block replaces your hardcoded headers.\n",
        "\n",
        "    header_row_1 = []\n",
        "    header_row_2 = []\n",
        "    header_row_3 = []\n",
        "\n",
        "    # Track headers to avoid duplicates in a row\n",
        "    last_h1 = None\n",
        "    last_h2 = None\n",
        "\n",
        "    for l0, l1, l2 in df.columns:\n",
        "        # Level 0 Header (e.g., 'Commitment Management')\n",
        "        if l0 != last_h1:\n",
        "            header_row_1.append(create_header_paragraph(l0))\n",
        "            last_h1 = l0\n",
        "        else:\n",
        "            header_row_1.append('')\n",
        "\n",
        "        # Level 1 Header (e.g., 'Affected Areas or Processes')\n",
        "        if l1 != last_h2:\n",
        "            header_row_2.append(create_header_paragraph(l1))\n",
        "            last_h2 = l1\n",
        "        else:\n",
        "            header_row_2.append('')\n",
        "\n",
        "        # Level 2 Header (e.g., the vertical text)\n",
        "        if l2: # Only add if Level 2 exists\n",
        "            header_row_3.append(VerticalText(l2))\n",
        "        else:\n",
        "            header_row_3.append('')\n",
        "\n",
        "    # --- DYNAMIC DATA ROW GENERATION ---\n",
        "    # This block replaces your hardcoded data_rows.\n",
        "    data_rows = []\n",
        "    for index, row in df.iterrows():\n",
        "        pdf_row = [create_data_paragraph(cell) for cell in row]\n",
        "        data_rows.append(pdf_row)\n",
        "\n",
        "    table_data = [header_row_1, header_row_2, header_row_3] + data_rows\n",
        "\n",
        "    dynamic_styles = []\n",
        "\n",
        "    level0_headers = df.columns.get_level_values(0)\n",
        "    start_col = 0\n",
        "    for i in range(1, len(level0_headers)):\n",
        "        if level0_headers[i] != level0_headers[i-1]:\n",
        "            dynamic_styles.append(('SPAN', (start_col, 0), (i - 1, 0)))\n",
        "            start_col = i\n",
        "    dynamic_styles.append(('SPAN', (start_col, 0), (len(level0_headers) - 1, 0)))\n",
        "\n",
        "    level1_headers = df.columns.droplevel(2)\n",
        "    start_col = 0\n",
        "    for i in range(1, len(level1_headers)):\n",
        "        if level1_headers[i] != level1_headers[i-1]:\n",
        "            dynamic_styles.append(('SPAN', (start_col, 1), (i - 1, 1)))\n",
        "            start_col = i\n",
        "    dynamic_styles.append(('SPAN', (start_col, 1), (len(level1_headers) - 1, 1)))\n",
        "\n",
        "    for i, (l0, l1, l2) in enumerate(df.columns):\n",
        "        if not l1 and not l2: # This is a simple, non-nested column\n",
        "            dynamic_styles.append(('SPAN', (i, 0), (i, 2)))\n",
        "        elif l1 and not l2: # Spans row 1 and 2\n",
        "            dynamic_styles.append(('SPAN', (i, 1), (i, 2)))\n",
        "\n",
        "    col_widths_proportions = [\n",
        "        0.05, 0.05, 0.04, 0.12, 0.04, 0.04, 0.03, 0.04, 0.04, 0.04, 0.04, 0.08,\n",
        "        0.06, 0.06, 0.04,\n",
        "        0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, # Affected Areas\n",
        "        0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, # Impact\n",
        "        0.06, 0.033\n",
        "    ]\n",
        "    col_widths = [p * table_width for p in col_widths_proportions]\n",
        "\n",
        "    # --- COMBINED TABLE STYLE ---\n",
        "    # Combine your static styles with our new dynamic SPAN styles\n",
        "    final_style = TableStyle([\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
        "        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
        "\n",
        "        # Header Row 1 (Top)\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#002060')),\n",
        "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
        "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
        "\n",
        "        # Header Rows 2 & 3\n",
        "        ('BACKGROUND', (0, 1), (-1, 2), colors.HexColor('#C00000')),\n",
        "        ('TEXTCOLOR', (0, 1), (-1, 2), colors.white),\n",
        "        ('FONTNAME', (0, 1), (-1, 2), 'Helvetica-Bold'),\n",
        "    ] + dynamic_styles) # Add the dynamically calculated SPANs\n",
        "\n",
        "    table = Table(table_data, colWidths=col_widths, repeatRows=3)\n",
        "    table.setStyle(final_style)\n",
        "    story = generate_commitment_register_second(output_filename)\n",
        "    doc.build( story +[PageBreak()]+ [table] , onFirstPage= header_footer , onLaterPages=header_footer)\n",
        "    print(f\"PDF successfully generated: {output_filename}\")\n",
        "\n",
        "\n",
        "# --- HOW TO USE THIS FUNCTION ---\n",
        "if __name__ == '__main__':\n",
        "    # This block is for demonstration. You would use your actual `df_final`.\n",
        "\n",
        "    # Re-create the structure of your df_final for a runnable example\n",
        "    header = [\n",
        "        ('Commitment Register Overview', 'Register Identifier', ''),\n",
        "        ('Commitment Register Overview', 'Commitment Identifier', ''),\n",
        "        ('Commitment Register Overview', 'Commitment or Obligation', ''),\n",
        "        ('Commitment Register Overview', 'Description', ''),\n",
        "        ('Commitment Register Overview', 'Project Phase', ''),\n",
        "        ('Commitment Management', 'Potential Impact on Scope?', ''),\n",
        "        ('Commitment Management', 'Status', ''),\n",
        "        ('Commitment Management', 'Commitment Deadline', ''),\n",
        "        ('Commitment Management', 'First Lead', ''),\n",
        "        ('Commitment Management', 'Second Lead', ''),\n",
        "        ('Commitment Management', 'Third Lead', ''),\n",
        "        ('Commitment Management', 'Primary Commitment Documentation', ''),\n",
        "        ('Commitment Management', 'Impact or Hazard Addressed', ''),\n",
        "        ('Commitment Management', 'Approving Agencies', ''),\n",
        "        ('Commitment Management', 'Other Stakeholders', ''),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Preparation/construction'),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Operation'),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Input Management'),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Discharge management'),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Off-Sites'),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Other'),\n",
        "        ('Commitment Management', 'Affected Areas or Processes', 'Fungibility'),\n",
        "        ('Commitment Management', 'Impact', 'CAPEX'),\n",
        "        ('Commitment Management', 'Impact', 'OPEX'),\n",
        "        ('Commitment Management', 'Impact', 'Health & Safety'),\n",
        "        ('Commitment Management', 'Impact', 'Social'),\n",
        "        ('Commitment Management', 'Impact', 'Economic'),\n",
        "        ('Commitment Management', 'Impact', 'Environmental'),\n",
        "        ('Commitment Management', 'Impact', 'Regulatory'),\n",
        "        ('Commitment Management', 'Comments', ''),\n",
        "        ('Commitment Management', 'Requires Change Order?', '')\n",
        "    ]\n",
        "generate_commitment_register_pdf(df_final)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYOr7xUDuttU",
        "outputId": "5630efee-0720-4418-91d7-ae60cd2a5512"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF successfully generated: commitment_register_final.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Header\n",
        "def header_footer(canvas, doc):\n",
        "    jesa_blue = colors.Color(red=0/255, green=51/255, blue=102/255)\n",
        "\n",
        "    header_offset = 0\n",
        "    canvas.saveState()\n",
        "    page_w, page_h = landscape(A3)\n",
        "\n",
        "    # Shift the header downward by subtracting offset\n",
        "    line_y = page_h - doc.topMargin + 2.15 * cm - header_offset\n",
        "    canvas.setStrokeColor(jesa_blue)\n",
        "    canvas.setLineWidth(1)\n",
        "    canvas.line(doc.leftMargin, line_y, page_w - doc.rightMargin, line_y)\n",
        "\n",
        "    # --- Top separator line ---\n",
        "    line_y = page_h - doc.topMargin + 0.5 * cm - header_offset\n",
        "    canvas.setStrokeColor(jesa_blue)\n",
        "    canvas.setLineWidth(1)\n",
        "    canvas.line(doc.leftMargin, line_y, page_w - doc.rightMargin, line_y)\n",
        "\n",
        "    # --- Logo ---\n",
        "    logo_path = 'jesa_logo.png'\n",
        "    logo_w = 4.0 * cm\n",
        "    logo_h = 3.0 * cm\n",
        "    logo_y = line_y + 0.2 * cm\n",
        "    if os.path.exists(logo_path):\n",
        "        logo = Image(logo_path, width=logo_w, height=logo_h)\n",
        "        logo.drawOn(canvas, doc.leftMargin, logo_y)\n",
        "    else:\n",
        "        canvas.setFont('Helvetica-Bold', 30)\n",
        "        canvas.setFillColor(jesa_blue)\n",
        "        canvas.drawString(doc.leftMargin, logo_y + 0.4*cm, \"JESA\")\n",
        "\n",
        "    # --- Left table ---\n",
        "    info_y = line_y - 0.2 * cm\n",
        "    left_data = [\n",
        "        ['Project Name:', 'Chemical additives plant'],\n",
        "        ['Customer:',     'NOVADDIX'],\n",
        "        ['Document Title:', 'Sustainable Project Delivery - Legal Register - Chemical additives plant']\n",
        "    ]\n",
        "    left_col_w = [(doc.width - logo_w - 0.5*cm)*0.1,\n",
        "                  (doc.width - logo_w - 0.5*cm)*0.8]\n",
        "\n",
        "    left_tbl = Table(left_data, colWidths=left_col_w)\n",
        "    left_tbl.setStyle(TableStyle([\n",
        "        ('FONTNAME',    (0,0), (0,-1),   'Helvetica-Bold'),\n",
        "        ('FONTNAME',    (1,0), (1,-1),   'Helvetica'),\n",
        "        ('FONTSIZE',    (0,0), (-1,-1),  6),\n",
        "        ('VALIGN',      (0,0), (-1,-1), 'TOP'),\n",
        "        ('LEFTPADDING', (0,0), (-1,-1), 0),\n",
        "        ('RIGHTPADDING',(0,0), (-1,-1), 0),\n",
        "    ]))\n",
        "    left_x = doc.leftMargin + logo_w + 0.5 * cm\n",
        "    left_tbl.wrapOn(canvas, doc.width, doc.topMargin)\n",
        "    left_tbl.drawOn(canvas, left_x, info_y)\n",
        "\n",
        "    # --- Right table ---\n",
        "    right_data = [\n",
        "        ['Q37440-00-EN-REG-00001'],\n",
        "        ['REV A'],\n",
        "        ['Page %d' % canvas.getPageNumber()]\n",
        "    ]\n",
        "    right_col_w = 3 * cm\n",
        "    right_tbl = Table(right_data, colWidths=[right_col_w])\n",
        "    right_tbl.setStyle(TableStyle([\n",
        "        ('FONTNAME',    (0,0), (-1,-1),   'Helvetica-Bold'),\n",
        "        ('FONTNAME',    (0,1), (0,1),     'Helvetica-Bold'),\n",
        "        ('FONTSIZE',    (0,0), (-1,-1),    6),\n",
        "        ('ALIGN',       (0,0), (-1,-1), 'RIGHT'),\n",
        "        ('LEFTPADDING', (0,0), (-1,-1), 0),\n",
        "        ('RIGHTPADDING',(0,0), (-1,-1), 0),\n",
        "    ]))\n",
        "    right_x = page_w - doc.rightMargin - right_col_w\n",
        "    right_tbl.wrapOn(canvas, doc.width, doc.topMargin)\n",
        "    right_tbl.drawOn(canvas, right_x, info_y)\n",
        "\n",
        "    canvas.restoreState()\n"
      ],
      "metadata": {
        "id": "-1uNNK0cvtlp"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Second page\n",
        "\n",
        "def generate_commitment_register_second(output_filename):\n",
        "    page_width, page_height = landscape(A3)\n",
        "\n",
        "    # Setup document with balanced side margins\n",
        "    doc = SimpleDocTemplate(\n",
        "        output_filename,\n",
        "        pagesize=landscape(A3),\n",
        "        rightMargin=1.2 * inch,\n",
        "        leftMargin = 1.2* inch,\n",
        "        topMargin=2.25 * inch,\n",
        "        bottomMargin=0.75 * inch,\n",
        "    )\n",
        "\n",
        "    # Use full content width for tables\n",
        "    usable_width = doc.width\n",
        "\n",
        "    story = []\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Custom color for JESA blue\n",
        "    jesa_blue = colors.Color(red=0/255, green=51/255, blue=102/255)\n",
        "\n",
        "\n",
        "    # Main heading style\n",
        "    styles.add(ParagraphStyle(\n",
        "        name='MainHeading',\n",
        "        fontName='Helvetica-Bold',\n",
        "        fontSize=8,  # Maintained as specified\n",
        "        leading=10,   # Tightened line spacing\n",
        "        textColor=jesa_blue,\n",
        "        spaceBefore=8,  # Added space above heading\n",
        "        spaceAfter=4    # Space below heading\n",
        "    ))\n",
        "\n",
        "    # Sub-heading style\n",
        "    styles.add(ParagraphStyle(\n",
        "        name='SubHeading',\n",
        "        fontName='Helvetica-Bold',\n",
        "        fontSize=7,   # Maintained as specified\n",
        "        leading=4,    # Tightened line spacing\n",
        "        textColor=colors.black,\n",
        "        spaceBefore=6,  # Space above subheading\n",
        "        spaceAfter=0    # Space below subheading\n",
        "    ))\n",
        "\n",
        "    # Body text style\n",
        "    body_style = styles['BodyText']\n",
        "    body_style.fontName = 'Helvetica'\n",
        "    body_style.fontSize = 7  # Maintained as specified\n",
        "    body_style.leading = 10  # Tightened line spacing\n",
        "    body_style.alignment = 4  # Justified text\n",
        "    body_style.spaceAfter = 4  # Reduced space after paragraph\n",
        "\n",
        "    # List item style\n",
        "    styles.add(ParagraphStyle(\n",
        "        name='ListItem',\n",
        "        parent=body_style,\n",
        "        leftIndent=0 * inch,\n",
        "        spaceBefore=0,\n",
        "        spaceAfter=0  # Space between list items\n",
        "    ))\n",
        "    # --- Build document content ---\n",
        "    story.append(Paragraph(\"1. Main purpose\", styles['MainHeading']))\n",
        "    story.append(Paragraph(\n",
        "        \"\"\"The Commitment Register is a system used to ensure commitments are incorporated into the appropriate part of engineering design, construction, procurement and/or operations, as required. Each commitment will be \"closed out\" in the Register before project phase completion, indicating that the commitment has been responsibly managed. A final Commitment Report is provided to the Customer at project phase completion outlining the inclusion of commitments into the various project documents and which commitments are compliant.\"\"\",\n",
        "        styles['BodyText']\n",
        "    ))\n",
        "\n",
        "    story.append(Paragraph(\"2. Definition\", styles['MainHeading']))\n",
        "    story.append(Paragraph(\n",
        "        \"An obligation is a requirement, under the law, necessary for compliance. Obligations and compliance are managed as part of Technical Integrity under SEAl. A commitment is a voluntary statement of action, or a goal, that goes beyond legal requirements. The Commitment Register for a project or contract lists the commitments made by the Customer in corporate or publicly available documentation. Typical sources include the Environmental Impact Assessment (EIA), Project Registers/Application or material published for the public in newspapers, open houses, etc.\",\n",
        "        styles['BodyText']\n",
        "    ))\n",
        "    story.append(Paragraph(\n",
        "        \"The Commitment Register is a central place to document, communicate, and track the commitments so they will be understood and included in the project. This Commitment Register is part of SEAl Sustainable Design Planning, which is described in the SEAl Standard (MS-E9-STD-00017). The Commitment Register should be discussed with the Customer before use on a project or contract as part of SEAl Alignment, including how commitments are to be recorded and managed while executing a project.\",\n",
        "        styles['BodyText']\n",
        "    ))\n",
        "    story.append(Paragraph(\n",
        "        \"As the project progresses, commitments may become obsolete or may not be feasible to implement within the project. The Commitment Register is used to track the status of all commitments including rationale for those commitments that become obsolete or are not feasible. These changes in status are tracked in the Commitment Register.\",\n",
        "        styles['BodyText']\n",
        "    ))\n",
        "\n",
        "    story.append(Paragraph(\"3. Initiation\", styles['MainHeading']))\n",
        "    story.append(Paragraph(\"Initiating and Customizing the Commitment Register\", styles['SubHeading']))\n",
        "    story.append(Paragraph(\"The Project Manager / Project Engineering Manager or designate, shall:\", styles['BodyText']))\n",
        "    story.append(Paragraph(\"- work with the Customer to populate the Register and classify the commitments.\", styles['ListItem']))\n",
        "    story.append(Paragraph(\"- be responsible for ensuring commitments are registered and communicated to the appropriate party (e.g. the discipline lead responsible for incorporating a given commitment within the project scope of work).\", styles['ListItem']))\n",
        "    story.append(Paragraph(\"The Commitment Register is designed to be customizable to suit the project's commitment tracking needs. Columns such as 'Affected areas or processes' should be customized to reflect the project.\", styles['BodyText']))\n",
        "\n",
        "    story.append(Paragraph(\"Register Maintenance\", styles['SubHeading']))\n",
        "    story.append(Paragraph(\"The Project Manager, Project Engineering Manager or designate, shall work with the Discipline Leads to maintain an accurate status of each commitment on the register. The register shall be updated as needed and controlled properly so only the most recent version is available to the project team. Sufficient hours shall be included in the project budget for register maintenance.\", styles['BodyText']))\n",
        "    story.append(Paragraph(\"Technical Review\", styles['SubHeading']))\n",
        "    story.append(Paragraph(\"The Commitment Register shall be reviewed by the Project Management Team and approved by the Customer at an agreed frequency for the project. After each review and approval the signed Commitment Register shall be converted to PDF and saved while updates continue in the live register.\", styles['BodyText']))\n",
        "    story.append(Paragraph(\"Other Considerations\", styles['SubHeading']))\n",
        "    story.append(Paragraph(\"The commitments and other registers (Legal, Sustainable Solutions Database), are normally created in conjunction with the Sustainability Steering Committee (SSC). The SSC is comprised of sustainability stakeholders from the customer (e.g. public relations, environmental advisors, regulatory contacts, operations manager) and JESA (e.g. Sustainability Lead, environmental scientists).\", styles['BodyText']))\n",
        "\n",
        "    story.append(Paragraph(\"4. References\", styles['MainHeading']))\n",
        "    for ref in [\n",
        "        \"Safe and Sustainable Engineering for Asset Lifecycle (SEAL) Standard (MS-E9-STD-00017)\",\n",
        "        \"Sustainable Project Delivery - Legal Register (MS-E9-TEM-00053)\",\n",
        "        \"Sustainable Solutions Standard (MS-FM-STD-00158)\",\n",
        "    ]:\n",
        "        story.append(Paragraph(ref, styles['ListItem']))\n",
        "    story.append(Spacer(1, 0.2 * inch))\n",
        "\n",
        "    story.append(Paragraph(\"5. Abbreviations\", styles['MainHeading']))\n",
        "    final_table_data = [\n",
        "        ['ABH', 'Agence du Bassin Hydraulique', 'EHS', 'Environment, Health & Safety'],\n",
        "        ['BAT', 'Best Available Technologies', 'HR', 'Human Resources'],\n",
        "        ['CRI', 'Centre R√©gional d\\'Investissement', 'IASE', 'Health Safety & Environment'],\n",
        "        ['ONG', 'Organisation Non Gouvernementale', 'PSE', 'Programme de Suivi et de Surveillance Environnemental'],\n",
        "        ['OCP', 'Office Ch√©rifien des Phosphates', 'SDG', 'Sustainable Development Goals'],\n",
        "        ['', '', 'SEAL', 'Safe and Sustainable Engineering for Asset Lifecycle'],\n",
        "    ]\n",
        "    abbrev_col_widths = [usable_width * 0.10, usable_width * 0.35, usable_width * 0.10, usable_width * 0.45]\n",
        "    abbreviations_table = Table(final_table_data, colWidths=abbrev_col_widths)\n",
        "    abbreviations_table.setStyle(TableStyle([\n",
        "        ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
        "        ('FONTSIZE', (0, 0), (-1, -1), 6),\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
        "        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
        "        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
        "        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
        "        ('FONTNAME', (2, 0), (2, -1), 'Helvetica-Bold'),\n",
        "        ('LEFTPADDING', (0,0), (-1,-1), 5),\n",
        "        ('RIGHTPADDING', (0,0), (-1,-1), 5),\n",
        "        ('TOPPADDING', (0,0), (-1,-1), 3),\n",
        "        ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n",
        "    ]))\n",
        "    story.append(abbreviations_table)\n",
        "    return story\n"
      ],
      "metadata": {
        "id": "ShbHGIHEzgdv"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cover page\n",
        "def generate_commitment_register_cover_page (PDF_PATH = \"commitment_register_cover_page.pdf\"):\n",
        "\n",
        "  LOGO_PATH = \"jesa_logo.png\"\n",
        "  PAGE_WIDTH, PAGE_HEIGHT = A4\n",
        "\n",
        "  # --- 2. Create a Dummy Logo if it doesn't exist ---\n",
        "  # This helps the script run even if the logo file is missing.\n",
        "  if not os.path.exists(LOGO_PATH):\n",
        "      try:\n",
        "          from PIL import Image as PILImage, ImageDraw, ImageFont\n",
        "          img = PILImage.new('RGB', (240, 70), color='white')\n",
        "          d = ImageDraw.Draw(img)\n",
        "          # Use a common bold font if available, otherwise default\n",
        "          try:\n",
        "              font = ImageFont.truetype(\"arialbd.ttf\", 50)\n",
        "          except IOError:\n",
        "              font = ImageFont.load_default()\n",
        "          d.text((10, 5), \"JESA\", fill=colors.HexColor(\"#1F497D\"), font=font)\n",
        "          img.save(LOGO_PATH)\n",
        "          print(f\"Created a dummy logo: {LOGO_PATH}\")\n",
        "      except Exception as e:\n",
        "          print(f\"Warning: Could not create a dummy logo. Please provide {LOGO_PATH}. Error: {e}\")\n",
        "\n",
        "  # --- 3. Document Setup ---\n",
        "  doc = SimpleDocTemplate(\n",
        "          PDF_PATH,\n",
        "          pagesize=A4,\n",
        "          rightMargin=0.5 * inch,\n",
        "          leftMargin = 0.5* inch,\n",
        "          topMargin=0.5 * inch,\n",
        "          bottomMargin=0.75 * inch,\n",
        "      )\n",
        "  elements = []\n",
        "  # Calculate the available width for content on the page\n",
        "  content_width = PAGE_WIDTH - doc.leftMargin - doc.rightMargin\n",
        "\n",
        "  # --- 4. Define Paragraph Styles ---\n",
        "  # Style for the main title in the top blue bar\n",
        "  header_style = ParagraphStyle(\n",
        "      name=\"Header\",\n",
        "      fontName=\"Helvetica-Bold\",\n",
        "      fontSize=12,\n",
        "      textColor=colors.white,\n",
        "      leading=16,\n",
        "      leftIndent=10\n",
        "  )\n",
        "\n",
        "  # Style for the \"Purpose of this register...\" text\n",
        "  subheader_style = ParagraphStyle(\n",
        "      name=\"Subheader\",\n",
        "      fontName=\"Helvetica\",\n",
        "      fontSize=7,\n",
        "      textColor=colors.black,\n",
        "      leading=12,\n",
        "      spaceAfter=6,\n",
        "  )\n",
        "\n",
        "  # Style for the blue field labels (e.g., \"PROJECT No:\")\n",
        "  label_style = ParagraphStyle(\n",
        "      name=\"Label\",\n",
        "      fontName=\"Helvetica-Bold\",\n",
        "      fontSize=6,\n",
        "      textColor=colors.white,\n",
        "      leftIndent=4,\n",
        "      leading=12,\n",
        "  )\n",
        "\n",
        "  # Style for the text inside the value boxes (e.g., \"Q37440\")\n",
        "  value_style = ParagraphStyle(\n",
        "      name=\"Value\",\n",
        "      fontName=\"Helvetica-bold\",\n",
        "      fontSize=6,\n",
        "      textColor=colors.black,\n",
        "      leading=12,\n",
        "  )\n",
        "\n",
        "  # Style for table text\n",
        "  table_style = ParagraphStyle(\n",
        "      name=\"TableText\",\n",
        "      fontName=\"Helvetica\",\n",
        "      fontSize=8,\n",
        "      textColor=colors.black,\n",
        "      leading=10,\n",
        "      alignment=1,  # Center alignment\n",
        "  )\n",
        "\n",
        "  table_header_style = ParagraphStyle(\n",
        "      name=\"TableHeader\",\n",
        "      fontName=\"Helvetica-Bold\",\n",
        "      fontSize=8,\n",
        "      textColor=colors.white,\n",
        "      leading=10,\n",
        "      alignment=1,  # Center alignment\n",
        "  )\n",
        "\n",
        "  # --- 5. Build Combined Header and Purpose Box ---\n",
        "  # This single table creates the continuous border effect.\n",
        "  title_para = Paragraph(\n",
        "      \"Sustainable Project Delivery - Legal Register - Chemical additives plant\",\n",
        "      header_style\n",
        "  )\n",
        "  logo_img = Image(LOGO_PATH, width=80, height=25)\n",
        "  purpose_para = Paragraph(\n",
        "      \"Purpose of this register is to record the regulatory requirements that need to be complied with by the project. \"\n",
        "      \"The register provides traceability of the action that has been taken to address the requirement.\",\n",
        "      subheader_style\n",
        "  )\n",
        "\n",
        "  # The table has two rows: one for the header, one for the purpose text.\n",
        "  combined_header_table = Table(\n",
        "      [\n",
        "          [title_para, logo_img],     # First row: Title and Logo\n",
        "          [purpose_para, None]        # Second row: Purpose text (spans both columns)\n",
        "      ],\n",
        "      colWidths=[content_width - 88, 88],\n",
        "      rowHeights=[12*mm, None] # First row has fixed height, second is auto\n",
        "  )\n",
        "\n",
        "  combined_header_table.setStyle(TableStyle([\n",
        "      # Span the purpose cell across the whole width\n",
        "      ('SPAN', (0, 1), (1, 1)),\n",
        "\n",
        "      # Background Colors\n",
        "      ('BACKGROUND', (0, 0), (0, 0), colors.HexColor(\"#1F497D\")), # Blue for title\n",
        "      ('BACKGROUND', (1, 0), (1, 0), colors.white),             # White for logo\n",
        "\n",
        "      # Alignment\n",
        "      ('VALIGN', (0, 0), (-1, 0), 'MIDDLE'), # Middle-align the header row\n",
        "      ('ALIGN', (1, 0), (1, 0), 'CENTER'),   # Center the logo\n",
        "\n",
        "      # Borders and Lines\n",
        "      ('BOX', (0, 0), (-1, -1), 1, colors.black), # Main outer border\n",
        "      ('LINEBELOW', (0, 0), (-1, 0), 1, colors.black), # Line under the header\n",
        "      ('LINEBEFORE', (1, 0), (1, 0), 1, colors.black), # Vertical line next to logo\n",
        "\n",
        "      # Padding for the purpose text cell\n",
        "      ('TOPPADDING', (0, 1), (-1, 1), 5),\n",
        "      ('BOTTOMPADDING', (0, 1), (-1, 1), 5),\n",
        "      ('LEFTPADDING', (0, 1), (-1, 1), 5),\n",
        "      ('RIGHTPADDING', (0, 1), (-1, 1), 5),\n",
        "  ]))\n",
        "\n",
        "  elements.append(combined_header_table)\n",
        "  elements.append(Spacer(1, 8*mm))\n",
        "\n",
        "  # --- 6. Build Project Detail Fields ---\n",
        "  fields = [\n",
        "      (\"PROJECT No:\", \"Q37440\"),\n",
        "      (\"PROJECT TITLE:\", \"Chemical additives plant\"),\n",
        "      (\"JESA DOCUMENT No:\", \"Q37440-00-EN-REG-00001\"),\n",
        "      (\"ELECTRONIC FILE LOCATION:\", \"N/A\"),\n",
        "      (\"NOTES:\", \"N/A\"),\n",
        "  ]\n",
        "\n",
        "  for label, val in fields:\n",
        "      # Create the full-width blue bar for the label\n",
        "      label_para = Paragraph(label, label_style)\n",
        "      label_table = Table([[label_para]], colWidths=[content_width], rowHeights=[7*mm])\n",
        "      label_table.setStyle(TableStyle([\n",
        "          ('BACKGROUND', (0, 0), (-1, -1), colors.HexColor(\"#1F497D\")),\n",
        "          ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
        "      ]))\n",
        "      elements.append(label_table)\n",
        "      elements.append(Spacer(1, 1.5*mm)) # Small space between label and value box\n",
        "\n",
        "      # Create the smaller, bordered box for the value\n",
        "      value_para = Paragraph(val, value_style)\n",
        "      # The value box has a fixed width (50% of the page content width)\n",
        "      value_box = Table([[value_para]], colWidths=[content_width / 2])\n",
        "      value_box.setStyle(TableStyle([\n",
        "          ('BOX', (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "          ('LEFTPADDING', (0, 0), (-1, -1), 4),\n",
        "          ('TOPPADDING', (0, 0), (-1, -1), 2),\n",
        "          ('BOTTOMPADDING', (0, 0), (-1, -1), 4),\n",
        "      ]))\n",
        "      elements.append(value_box)\n",
        "\n",
        "      # Add a larger space before the next field starts\n",
        "      elements.append(Spacer(1, 5*mm))\n",
        "\n",
        "  # --- 7. Add Bottom Status Table ---\n",
        "  # Add significant space to push content towards bottom of page\n",
        "  elements.append(Spacer(1, 40*mm))\n",
        "\n",
        "  # Create the Originator/Issue Date table (first row)\n",
        "  originator_table_data = [\n",
        "      [\n",
        "          Paragraph(\"Originator:\", table_style),\n",
        "          Paragraph(\"Y.Hosni\", table_style),\n",
        "          Paragraph(\"Issue Date:\", table_style),\n",
        "          Paragraph(\"18-Jun-25\", table_style)\n",
        "      ]\n",
        "  ]\n",
        "\n",
        "  originator_table = Table(\n",
        "      originator_table_data,\n",
        "      colWidths=[content_width * 0.15, content_width * 0.35, content_width * 0.15, content_width * 0.35]\n",
        "  )\n",
        "\n",
        "  originator_table.setStyle(TableStyle([\n",
        "      ('BOX', (0, 0), (-1, -1), 1, colors.black),\n",
        "      ('INNERGRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
        "      ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
        "      ('LEFTPADDING', (0, 0), (-1, -1), 3),\n",
        "      ('RIGHTPADDING', (0, 0), (-1, -1), 3),\n",
        "      ('TOPPADDING', (0, 0), (-1, -1), 4),\n",
        "      ('BOTTOMPADDING', (0, 0), (-1, -1), 4),\n",
        "  ]))\n",
        "\n",
        "  elements.append(originator_table)\n",
        "\n",
        "  # Create the Document Status header\n",
        "  status_header_data = [\n",
        "      [Paragraph(\"DOCUMENT STATUS\", table_header_style)]\n",
        "  ]\n",
        "\n",
        "  status_header_table = Table(\n",
        "      status_header_data,\n",
        "      colWidths=[content_width]\n",
        "  )\n",
        "\n",
        "  status_header_table.setStyle(TableStyle([\n",
        "      ('BACKGROUND', (0, 0), (-1, -1), colors.HexColor(\"#1F497D\")),\n",
        "      ('BOX', (0, 0), (-1, -1), 1, colors.black),\n",
        "      ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
        "      ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "      ('TOPPADDING', (0, 0), (-1, -1), 6),\n",
        "      ('BOTTOMPADDING', (0, 0), (-1, -1), 6),\n",
        "  ]))\n",
        "\n",
        "  elements.append(status_header_table)\n",
        "\n",
        "  # Create the main status table with headers and data\n",
        "  status_table_data = [\n",
        "      # Data row B\n",
        "      [\n",
        "          Paragraph(\"B\", table_style),\n",
        "          Paragraph(\"18-Jun-25\", table_style),\n",
        "          Paragraph(\"Issued for Review (IFR)\", table_style),\n",
        "          Paragraph(\"Y.Hosni\", table_style),\n",
        "          Paragraph(\"S.El Alem\", table_style),\n",
        "          Paragraph(\"J.Alaoui Sosse\", table_style),\n",
        "          Paragraph(\"S. Paresh\", table_style)\n",
        "      ],\n",
        "      # Data row A\n",
        "      [\n",
        "          Paragraph(\"A\", table_style),\n",
        "          Paragraph(\"11-Mar-25\", table_style),\n",
        "          Paragraph(\"Issued for Internal Review (IIR)\", table_style),\n",
        "          Paragraph(\"I.Issa Issaka\", table_style),\n",
        "          Paragraph(\"S.El Alem\", table_style),\n",
        "          Paragraph(\"J.Alaoui Sosse\", table_style),\n",
        "          Paragraph(\"S. Salim\", table_style)\n",
        "      ],\n",
        "      [\n",
        "          Paragraph(\"REV\", table_style),\n",
        "          Paragraph(\"DATE\", table_style),\n",
        "          Paragraph(\"DESCRIPTION\", table_style),\n",
        "          Paragraph(\"BY\", table_style),\n",
        "          Paragraph(\"CHKD\", table_style),\n",
        "          Paragraph(\"D.APPD\", table_style),\n",
        "          Paragraph(\"P.APPD\", table_style)\n",
        "      ],\n",
        "  ]\n",
        "\n",
        "  status_table = Table(\n",
        "      status_table_data,\n",
        "      colWidths=[\n",
        "          content_width * 0.06,   # REV\n",
        "          content_width * 0.12,   # DATE\n",
        "          content_width * 0.35,   # DESCRIPTION\n",
        "          content_width * 0.15,   # BY\n",
        "          content_width * 0.12,   # CHKD\n",
        "          content_width * 0.12,   # D.APPD\n",
        "          content_width * 0.08    # P.APPD\n",
        "      ]\n",
        "  )\n",
        "  num_rows = len(status_table_data)\n",
        "\n",
        "  status_table.setStyle(TableStyle([\n",
        "      ('BOX', (0, 0), (-1, -1), 1, colors.black),\n",
        "      ('INNERGRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
        "      ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
        "      ('LEFTPADDING', (0, 0), (-1, -1), 3),\n",
        "      ('RIGHTPADDING', (0, 0), (-1, -1), 3),\n",
        "      ('TOPPADDING', (0, 0), (-1, -1), 4),\n",
        "      ('BOTTOMPADDING', (0, 0), (-1, -1), 4),\n",
        "      # Light blue background for header row only\n",
        "      ('BACKGROUND', (0, num_rows - 1), (-1, num_rows - 1), colors.HexColor(\"#E6F3FF\")),\n",
        "\n",
        "  ]))\n",
        "\n",
        "  elements.append(status_table)\n",
        "\n",
        "  # Copyright notice\n",
        "  copyright_para = Paragraph(\n",
        "      \"¬© Copyright 2021 JESA Group. No part of this document or the information it contains may be reproduced or transmitted in any form or by any means electronic or mechanical, including photocopying, recording, or by any information storage and retrieval system, without permission in writing from JESA. JESA.com\",\n",
        "      ParagraphStyle(\n",
        "          name=\"Copyright\",\n",
        "          fontName=\"Helvetica-bold\",\n",
        "          fontSize=7,\n",
        "          textColor=colors.black,\n",
        "          leading=9,\n",
        "          alignment=0,\n",
        "          spaceAfter=0,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  elements.append(Spacer(1, 4*mm))\n",
        "  elements.append(copyright_para)\n",
        "\n",
        "  # --- 8. Render the PDF ---\n",
        "  doc.build(elements)\n",
        "  print(f\"‚úÖ PDF successfully created: {PDF_PATH}\")\n",
        "generate_commitment_register_cover_page()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6iDX7GZAmgc",
        "outputId": "5665d7d4-bc2f-448f-d15e-3823bfb53a15"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PDF successfully created: commitment_register_cover_page.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Final Commitment Register\n",
        "from PyPDF2 import PdfMerger\n",
        "\n",
        "def build_full_pdf(df_final ,output_name = \"commitment_register.pdf\" ):\n",
        "    # 1. Generate A4 page\n",
        "    generate_commitment_register_cover_page()\n",
        "\n",
        "    generate_commitment_register_pdf(df_final)\n",
        "\n",
        "    # 3. Merge both into one\n",
        "    merger = PdfMerger()\n",
        "    merger.append(\"commitment_register_cover_page.pdf\")\n",
        "    merger.append(\"commitment_register_final.pdf\")\n",
        "    merger.write(output_name)\n",
        "    merger.close()\n",
        "\n",
        "    print(\"‚úÖ full_commitment_register.pdf successfully created\")\n",
        "\n",
        "build_full_pdf(df_final)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of54M_LmCXUd",
        "outputId": "3d0e4e52-a833-4932-95c8-0e0c470ca42e"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PDF successfully created: commitment_register_cover_page.pdf\n",
            "PDF successfully generated: commitment_register_final.pdf\n",
            "‚úÖ full_commitment_register.pdf successfully created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "import asyncio\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import traceback\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "\n",
        "\n",
        "# Import your web scraping functions\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "# Apply nest_asyncio for Jupyter/Colab compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class EnvironmentalComplianceProcessor:\n",
        "    def __init__(self):\n",
        "        self.pdf_links = set()\n",
        "        self.knowledge_base = None\n",
        "        self.parsed_results = None\n",
        "        self.commitment_df = None\n",
        "\n",
        "    async def scrape_pdfs_from_url(self, base_url, max_pages=4):\n",
        "        \"\"\"Scrape PDFs from the provided URL\"\"\"\n",
        "        pdf_links = set()\n",
        "\n",
        "        async def extract_pdfs_from_html(html):\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            new_links = set()\n",
        "\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                href = a[\"href\"]\n",
        "                if href.lower().endswith(\".pdf\"):\n",
        "                    full_link = urljoin(base_url, href)\n",
        "                    new_links.add(full_link)\n",
        "\n",
        "            return new_links\n",
        "\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True)\n",
        "            context = await browser.new_context(user_agent=(\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "                \"(KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
        "            ))\n",
        "            page = await context.new_page()\n",
        "\n",
        "            try:\n",
        "                await page.goto(base_url, wait_until=\"networkidle\", timeout=30000)\n",
        "                await asyncio.sleep(3)\n",
        "\n",
        "                # Extract from main page\n",
        "                main_html = await page.content()\n",
        "                new_links = await extract_pdfs_from_html(main_html)\n",
        "                pdf_links.update(new_links)\n",
        "\n",
        "                # Extract from frames\n",
        "                frames = page.frames\n",
        "                for frame in frames:\n",
        "                    try:\n",
        "                        frame_html = await frame.content()\n",
        "                        frame_links = await extract_pdfs_from_html(frame_html)\n",
        "                        pdf_links.update(frame_links)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"Error scraping {base_url}: {str(e)}\")\n",
        "            finally:\n",
        "                await browser.close()\n",
        "\n",
        "        return list(pdf_links)\n",
        "\n",
        "    def download_pdfs(self, pdf_links, progress_callback=None):\n",
        "        \"\"\"Download PDFs from links to temporary directory\"\"\"\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        downloaded_files = []\n",
        "\n",
        "        for i, link in enumerate(pdf_links):\n",
        "            try:\n",
        "                if progress_callback:\n",
        "                    progress_callback((i / len(pdf_links)) * 0.3, f\"Downloading PDF {i+1}/{len(pdf_links)}\")\n",
        "\n",
        "                response = requests.get(link, timeout=30, headers={\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                })\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Extract filename from URL or create one\n",
        "                filename = os.path.basename(link.split('?')[0])\n",
        "                if not filename.endswith('.pdf'):\n",
        "                    filename = f\"document_{i+1}.pdf\"\n",
        "\n",
        "                filepath = os.path.join(temp_dir, filename)\n",
        "\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                downloaded_files.append(filepath)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download {link}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return downloaded_files\n",
        "\n",
        "    def parse_pdfs_to_knowledge_base(self, pdf_files, progress_callback=None):\n",
        "        \"\"\"Parse PDFs and create knowledge base\"\"\"\n",
        "        if progress_callback:\n",
        "            progress_callback(0.3, \"Initializing PDF parser...\")\n",
        "\n",
        "        # Configure parser\n",
        "        config = TableValidationConfig(\n",
        "            max_columns=10,\n",
        "            min_rows=2,\n",
        "            max_null_percentage=0.4,\n",
        "            min_content_ratio=0.3\n",
        "        )\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(0.4, f\"Processing {len(pdf_files)} PDF files...\")\n",
        "\n",
        "        # Process PDFs\n",
        "        batch_results = process_pdf_batch(pdf_files, config)\n",
        "        self.parsed_results = batch_results\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(0.6, \"Creating knowledge base...\")\n",
        "\n",
        "        # Create knowledge base\n",
        "        self.knowledge_base = self.create_knowledge_base_from_results(batch_results)\n",
        "\n",
        "        return batch_results\n",
        "\n",
        "    def create_knowledge_base_from_results(self, batch_results):\n",
        "        \"\"\"Create knowledge base from parsing results\"\"\"\n",
        "        knowledge_base = []\n",
        "\n",
        "        for doc_path, doc_data in batch_results['results'].items():\n",
        "            if 'error' in doc_data:\n",
        "                continue\n",
        "\n",
        "            full_text = \"\"\n",
        "            for page in doc_data.get('pages', []):\n",
        "                if 'content' in page:\n",
        "                    if 'text_structure' in page['content']:\n",
        "                        full_text += page['content']['text_structure'].get('title_text', '') + \"\\n\"\n",
        "                        full_text += page['content']['text_structure'].get('body_text', '') + \"\\n\"\n",
        "                    elif 'text' in page['content']:\n",
        "                        full_text += page['content']['text'] + \"\\n\"\n",
        "\n",
        "            if full_text.strip():\n",
        "                knowledge_base.append({\n",
        "                    \"filename\": doc_data.get('filename', 'N/A'),\n",
        "                    \"document_type\": doc_data.get('document_type', 'N/A'),\n",
        "                    \"content\": full_text.lower()\n",
        "                })\n",
        "\n",
        "        return knowledge_base\n",
        "\n",
        "    def process_commitments_with_ai(self, project_description, api_key, progress_callback=None):\n",
        "        \"\"\"Process commitment register with AI completion\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            raise Exception(\"No knowledge base available. Please process PDFs first.\")\n",
        "\n",
        "        if not api_key:\n",
        "            raise Exception(\"Gemini API key is required\")\n",
        "\n",
        "        # Configure Gemini API\n",
        "        import google.generativeai as genai\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(0.7, \"Creating commitment register...\")\n",
        "\n",
        "        # Create default commitments DataFrame\n",
        "        df = self.create_default_commitments_df()\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(0.75, \"Analyzing commitments with AI...\")\n",
        "\n",
        "        # Process each commitment\n",
        "        for index, row in df.iterrows():\n",
        "            commitment_description = row[('Commitment Register Overview', 'Description', '')]\n",
        "            commitment_id = row[('Commitment Register Overview', 'Commitment Identifier', '')]\n",
        "\n",
        "            if progress_callback:\n",
        "                progress = 0.75 + (0.15 * index / len(df))\n",
        "                progress_callback(progress, f\"Processing: {commitment_id}\")\n",
        "\n",
        "            # Find relevant documents\n",
        "            relevant_texts = self.find_relevant_documents(commitment_description)\n",
        "\n",
        "            if relevant_texts:\n",
        "                try:\n",
        "                    # Call Gemini to complete the row\n",
        "                    extracted_data = call_gemini_to_complete_row(row, relevant_texts, project_description)\n",
        "\n",
        "                    if extracted_data:\n",
        "                        # Update DataFrame with AI results\n",
        "                        self.update_commitment_row(df, index, extracted_data)\n",
        "\n",
        "                    # Small delay to respect API limits\n",
        "                    import time\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing commitment {commitment_id}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        self.commitment_df = df\n",
        "        return df\n",
        "\n",
        "    def find_relevant_documents(self, commitment_description):\n",
        "        \"\"\"Find relevant documents from knowledge base\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return \"\"\n",
        "\n",
        "        commitment_description = commitment_description.lower()\n",
        "        import re\n",
        "        keywords = set(re.findall(r'\\b[a-zA-Z√ß√©√†√®√π√¢√™√Æ√¥√ª√¶≈ì\\d]{4,}\\b', commitment_description))\n",
        "\n",
        "        relevant_texts = []\n",
        "        for doc in self.knowledge_base:\n",
        "            if any(keyword in doc['filename'].lower() for keyword in keywords) or \\\n",
        "               any(keyword in doc['content'] for keyword in keywords):\n",
        "                relevant_texts.append(f\"--- START OF RELEVANT DOCUMENT ({doc['filename']}) ---\\n{doc['content']}\\n--- END OF DOCUMENT ---\\n\")\n",
        "\n",
        "        return \"\\n\".join(relevant_texts)\n",
        "\n",
        "    def update_commitment_row(self, df, index, extracted_data):\n",
        "        \"\"\"Update DataFrame row with extracted data\"\"\"\n",
        "        field_mappings = {\n",
        "            \"Impact or Hazard Addressed\": ('Commitment Management', 'Impact or Hazard Addressed', ''),\n",
        "            \"Approving Agencies\": ('Commitment Management', 'Approving Agencies', ''),\n",
        "            \"Comments\": ('Commitment Management', 'Comments', ''),\n",
        "            \"Affected_Preparation_Construction\": ('Commitment Management', 'Affected Areas or Processes', 'Preparation/construction'),\n",
        "            \"Affected_Operation\": ('Commitment Management', 'Affected Areas or Processes', 'Operation'),\n",
        "            \"Affected_Discharge_Management\": ('Commitment Management', 'Affected Areas or Processes', 'Discharge management'),\n",
        "            \"Impact_Health_Safety\": ('Commitment Management', 'Impact', 'Health & Safety'),\n",
        "            \"Impact_Environmental\": ('Commitment Management', 'Impact', 'Environmental'),\n",
        "            \"Impact_Regulatory\": ('Commitment Management', 'Impact', 'Regulatory')\n",
        "        }\n",
        "\n",
        "        for key, column in field_mappings.items():\n",
        "            if key in extracted_data:\n",
        "                df.loc[index, column] = extracted_data[key]\n",
        "\n",
        "    def create_default_commitments_df(self):\n",
        "        \"\"\"Create default commitments DataFrame\"\"\"\n",
        "        header = [\n",
        "            ('Commitment Register Overview', 'Register Identifier', ''),\n",
        "            ('Commitment Register Overview', 'Commitment Identifier', ''),\n",
        "            ('Commitment Register Overview', 'Commitment or Obligation', ''),\n",
        "            ('Commitment Register Overview', 'Description', ''),\n",
        "            ('Commitment Register Overview', 'Project Phase', ''),\n",
        "            ('Commitment Management', 'Potential Impact on Scope?', ''),\n",
        "            ('Commitment Management', 'Status', ''),\n",
        "            ('Commitment Management', 'Commitment Deadline', ''),\n",
        "            ('Commitment Management', 'First Lead', ''),\n",
        "            ('Commitment Management', 'Second Lead', ''),\n",
        "            ('Commitment Management', 'Third Lead', ''),\n",
        "            ('Commitment Management', 'Primary Commitment Documentation', ''),\n",
        "            ('Commitment Management', 'Impact or Hazard Addressed', ''),\n",
        "            ('Commitment Management', 'Approving Agencies', ''),\n",
        "            ('Commitment Management', 'Other Stakeholders', ''),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Preparation/construction'),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Operation'),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Input Management'),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Discharge management'),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Off-Sites'),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Other'),\n",
        "            ('Commitment Management', 'Affected Areas or Processes', 'Fungibility'),\n",
        "            ('Commitment Management', 'Impact', 'CAPEX'),\n",
        "            ('Commitment Management', 'Impact', 'OPEX'),\n",
        "            ('Commitment Management', 'Impact', 'Health & Safety'),\n",
        "            ('Commitment Management', 'Impact', 'Social'),\n",
        "            ('Commitment Management', 'Impact', 'Economic'),\n",
        "            ('Commitment Management', 'Impact', 'Environmental'),\n",
        "            ('Commitment Management', 'Impact', 'Regulatory'),\n",
        "            ('Commitment Management', 'Comments', ''),\n",
        "            ('Commitment Management', 'Requires Change Order?', '')\n",
        "        ]\n",
        "\n",
        "        columns = pd.MultiIndex.from_tuples(header)\n",
        "\n",
        "        initial_commitments_data = [\n",
        "            [\"Moroccan environmental regulation\", \"Law n¬∞ 13-03 relating to the fight against air pollution\", \"Legal obligation\", \"Controlling the atmospheric emissions during industrial operations while ensuring good air quality.\", \"Design/Operation\", \"High\", \"In Progress\", \"During operational phase\", \"HSE Client\", \"Environment Client\", \"\", \"Environmental Report\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
        "            [\"OCP Group objectives and commitment\", \"Liquid effluents policy\", \"Commitment\", \"Complying with legal regulatory national and international requirements for liquid discharge to ensure the prevention and control of related environmental risks.\", \"Design/Construction/Operation\", \"High\", \"In Progress\", \"During Design, construction and operation phases\", \"Process Engineering\", \"Environmental Engineering\", \"Civil Engineering\", \"Environmental Design criteria; Liquid effluents policy (OCP)\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
        "            [\"OCP Group objectives and commitment\", \"Waste management policy\", \"Commitment\", \"Responsibly manage the waste generated by the project, respecting national and internationally recognized guidelines.\", \"Design/Construction/Operation\", \"High\", \"In Progress\", \"During design Construction and operation phases\", \"Environmental Engineering\", \"HSE Client\", \"Environment-Construction Contractors\", \"Waste management plan\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "        ]\n",
        "\n",
        "        return pd.DataFrame(initial_commitments_data, columns=columns)\n",
        "\n",
        "    def generate_output_files(self, progress_callback=None):\n",
        "        \"\"\"Generate final PDF and CSV files\"\"\"\n",
        "        if self.commitment_df is None:\n",
        "            raise Exception(\"No commitment data available\")\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(0.95, \"Generating output files...\")\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Generate CSV\n",
        "        csv_filename = f\"commitment_register_{timestamp}.csv\"\n",
        "        csv_path = os.path.join(tempfile.gettempdir(), csv_filename)\n",
        "        self.commitment_df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # Generate PDF\n",
        "        pdf_filename = f\"commitment_register_{timestamp}.pdf\"\n",
        "        pdf_path = os.path.join(tempfile.gettempdir(), pdf_filename)\n",
        "        build_full_pdf(self.commitment_df, pdf_path)\n",
        "\n",
        "        return csv_path, pdf_path\n",
        "\n",
        "    async def process_complete_workflow(self, url, project_description, api_key, max_pages=4, progress=gr.Progress()):\n",
        "        \"\"\"Complete workflow from URL to final outputs\"\"\"\n",
        "        try:\n",
        "            # Step 1: Scrape PDFs\n",
        "            progress(0.0, \"üîç Scraping PDFs from website...\")\n",
        "            pdf_links = await self.scrape_pdfs_from_url(url, max_pages)\n",
        "\n",
        "            if not pdf_links:\n",
        "                return \"‚ùå No PDF links found on the provided URL\", None, None, None\n",
        "\n",
        "            progress(0.1, f\"üìÑ Found {len(pdf_links)} PDF links\")\n",
        "\n",
        "            # Step 2: Download PDFs\n",
        "            progress(0.15, \"‚¨áÔ∏è Downloading PDF files...\")\n",
        "\n",
        "            def download_progress(prog, desc):\n",
        "                progress(0.15 + prog * 0.15, desc)\n",
        "\n",
        "            pdf_files = self.download_pdfs(pdf_links, download_progress)\n",
        "\n",
        "            if not pdf_files:\n",
        "                return \"‚ùå Failed to download any PDF files\", None, None, None\n",
        "\n",
        "            # Step 3: Parse PDFs\n",
        "            progress(0.3, \"üî¨ Parsing PDF documents...\")\n",
        "\n",
        "            def parse_progress(prog, desc):\n",
        "                progress(0.3 + prog * 0.4, desc)\n",
        "\n",
        "            parsing_results = self.parse_pdfs_to_knowledge_base(pdf_files, parse_progress)\n",
        "\n",
        "            # Step 4: Process with AI\n",
        "            progress(0.7, \"ü§ñ Processing commitments with AI...\")\n",
        "\n",
        "            def ai_progress(prog, desc):\n",
        "                progress(0.7 + prog * 0.25, desc)\n",
        "\n",
        "            commitment_df = self.process_commitments_with_ai(project_description, api_key, ai_progress)\n",
        "\n",
        "            # Step 5: Generate outputs\n",
        "            progress(0.95, \"üìä Generating final outputs...\")\n",
        "            csv_path, pdf_path = self.generate_output_files(lambda p, d: progress(0.95 + p * 0.05, d))\n",
        "\n",
        "            progress(1.0, \"‚úÖ Complete!\")\n",
        "\n",
        "            # Create summary\n",
        "            summary = f\"\"\"\n",
        "‚úÖ **Workflow Complete!**\n",
        "\n",
        "üìä **Processing Summary:**\n",
        "- PDFs found: {len(pdf_links)}\n",
        "- PDFs downloaded: {len(pdf_files)}\n",
        "- PDFs successfully parsed: {parsing_results['summary']['successful']}\n",
        "- Knowledge base documents: {len(self.knowledge_base)}\n",
        "- Commitments processed: {len(commitment_df)}\n",
        "- Average parsing confidence: {parsing_results['summary']['avg_confidence']:.2f}\n",
        "\n",
        "üéØ **Results:**\n",
        "- Commitment register completed with AI analysis\n",
        "- Environmental compliance requirements identified\n",
        "- Regulatory references integrated\n",
        "- Impact assessments completed\n",
        "            \"\"\"\n",
        "\n",
        "            return summary, csv_path, pdf_path, json.dumps(parsing_results, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error in workflow:** {str(e)}\\n\\n**Traceback:**\\n{traceback.format_exc()}\"\n",
        "            return error_msg, None, None, None\n",
        "\n",
        "# Create the processor instance\n",
        "processor = EnvironmentalComplianceProcessor()\n",
        "\n",
        "# Define the streamlined Gradio interface\n",
        "with gr.Blocks(title=\"Environmental Compliance Processor\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style=\"text-align: center; padding: 20px; background: linear-gradient(90deg, #4CAF50, #2196F3); border-radius: 10px; margin: 10px;\">\n",
        "        <h1 style=\"color: white; margin: 0;\">üå± Environmental Compliance Processor</h1>\n",
        "        <p style=\"color: white; margin: 5px 0;\">Automated workflow: URL ‚Üí PDF Scraping ‚Üí Parsing ‚Üí AI Analysis ‚Üí Report Generation</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üîó Input Configuration\")\n",
        "\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"Website URL to Scrape\",\n",
        "                placeholder=\"https://environnement.gov.ma/fr/lois-et-reglementations/normes\",\n",
        "                value=\"https://environnement.gov.ma/fr/lois-et-reglementations/normes\",\n",
        "                info=\"Enter the URL containing environmental PDF documents\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                max_pages_input = gr.Number(\n",
        "                    value=4,\n",
        "                    label=\"Max Pages\",\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    info=\"Maximum pages to scrape\"\n",
        "                )\n",
        "\n",
        "                api_key_input = gr.Textbox(\n",
        "                    label=\"Gemini API Key\",\n",
        "                    type=\"password\",\n",
        "                    placeholder=\"Enter your Google Gemini API key\",\n",
        "                    info=\"Required for AI-powered analysis\"\n",
        "                )\n",
        "\n",
        "            project_description = gr.Textbox(\n",
        "                label=\"Project Description\",\n",
        "                lines=6,\n",
        "                value=\"\"\"JESA has entered a reimbursable Work Order for elaborating the FEED (Evaluate+ Define) for this project.\n",
        "This revision includes ESIA preparation, architectural activities for non-process buildings and master plan.\n",
        "Division of Responsibilities: JESA has an EPCM reimbursable scope, with Customer coordination with authorities.\n",
        "Project Objectives: Phase 2 development study and Class 4 estimate (+/- 20% to +/- 30%).\n",
        "Activities include: Civil early works, Geo scan, Storage building, Environmental deliverables, and Mechanical ITBs.\"\"\",\n",
        "                info=\"Context for AI analysis of commitments\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\n",
        "                \"üöÄ Start Complete Processing\",\n",
        "                variant=\"primary\",\n",
        "                size=\"lg\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìä Results\")\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"Processing Status\",\n",
        "                lines=15,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                csv_download = gr.File(\n",
        "                    label=\"üìÑ Download CSV Report\",\n",
        "                    visible=True\n",
        "                )\n",
        "\n",
        "                pdf_download = gr.File(\n",
        "                    label=\"üìÑ Download PDF Report\",\n",
        "                    visible=True\n",
        "                )\n",
        "\n",
        "            with gr.Accordion(\"üîç Detailed JSON Results\", open=False):\n",
        "                json_output = gr.Textbox(\n",
        "                    label=\"Raw Parsing Results (JSON)\",\n",
        "                    lines=10,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "    # Event handler for the main processing button\n",
        "    async def process_workflow_wrapper(url, project_desc, api_key, max_pages):\n",
        "        return await processor.process_complete_workflow(\n",
        "            url, project_desc, api_key, max_pages\n",
        "        )\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=process_workflow_wrapper,\n",
        "        inputs=[url_input, project_description, api_key_input, max_pages_input],\n",
        "        outputs=[status_output, csv_download, pdf_download, json_output]\n",
        "    )\n",
        "\n",
        "    # Add examples section\n",
        "    with gr.Accordion(\"üí° Usage Examples\", open=False):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### Example URLs to try:\n",
        "\n",
        "        - **Moroccan Environmental Regulations:** `https://environnement.gov.ma/fr/lois-et-reglementations/normes`\n",
        "        - **Ministry of Energy:** `https://www.mem.gov.ma/fr/Pages/secteur.aspx?e=3`\n",
        "        - **Water Resources:** `https://www.water.gov.ma/reglementation/`\n",
        "\n",
        "        ### What the system does:\n",
        "\n",
        "        1. **üîç Web Scraping:** Automatically finds and downloads PDF documents from the provided URL\n",
        "        2. **üî¨ PDF Analysis:** Uses advanced parsing to extract text, tables, and regulatory information\n",
        "        3. **ü§ñ AI Processing:** Leverages Gemini AI to analyze commitments against environmental regulations\n",
        "        4. **üìä Report Generation:** Creates professional PDF and CSV reports with compliance analysis\n",
        "\n",
        "        ### Requirements:\n",
        "        - Valid Google Gemini API key\n",
        "        - URL containing environmental PDF documents\n",
        "        - Internet connection for scraping and AI processing\n",
        "        \"\"\")\n",
        "\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style=\"text-align: center; padding: 10px; color: #666;\">\n",
        "        <small>üîß System Status: Ready for Environmental Compliance Processing</small>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sjq0kQasF1yS",
        "outputId": "3042cec1-a96a-422a-efed-d91de2d8df06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8a90f32c97671378bb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8a90f32c97671378bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:√âchec OCR page: type object 'Image' has no attribute 'open'\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n",
            "WARNING:__main__:√âchec OCR page: type object 'Image' has no attribute 'open'\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n",
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-93' coro=<Queue.start_progress_updates() running at /usr/local/lib/python3.11/dist-packages/gradio/queueing.py:359> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-88' coro=<_delete_state() running at /usr/local/lib/python3.11/dist-packages/gradio/route_utils.py:980> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-92' coro=<Queue.start_processing() running at /usr/local/lib/python3.11/dist-packages/gradio/queueing.py:309> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
            "WARNING:__main__:√âchec OCR page: type object 'Image' has no attribute 'open'\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n",
            "WARNING:__main__:√âchec OCR page: type object 'Image' has no attribute 'open'\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Document principalement scann√© - consid√©rer une version native si disponible\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n",
            "WARNING:__main__:  Recommandations:\n",
            "WARNING:__main__:    - Confiance faible - r√©vision manuelle recommand√©e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PDF successfully created: commitment_register_cover_page.pdf\n",
            "PDF successfully generated: commitment_register_final.pdf\n",
            "‚úÖ full_commitment_register.pdf successfully created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJ9mwk0ZPJIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}